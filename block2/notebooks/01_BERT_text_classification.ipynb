{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0ff49d-705a-4e8d-833c-aa037e013e33",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "First Try: We fine-tune BERT as text classifier: with finetune everything strategy\n",
    "\n",
    "1. Download 20-newsgroup data set from sklearn\n",
    "2. Load pretrained BERT from hugging face\n",
    "3. Train BERT as classifier\n",
    "\n",
    "Second Try: We fine-tune a text classifier based on the embeddings from BERT.\n",
    "\n",
    "1. Download 20-newsgroup data set from sklearn\n",
    "2. Load pretrained BERT from hugging face and use BERT for text embedding\n",
    "3. Define a classifier\n",
    "4. Train the classifier for embedded text input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226286c2-b4f6-4441-8e39-6d92d3c3aa5a",
   "metadata": {},
   "source": [
    "## 1. Load Data from sklearn and create Dataset and DataLoader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a453f790-3980-4e23-915a-7d2c4999b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(data_home=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400af912-ecca-4012-9f8b-79dcf80f8f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "=================   ==========\n",
      "Classes                     20\n",
      "Samples total            18846\n",
      "Dimensionality               1\n",
      "Features                  text\n",
      "=================   ==========\n",
      "\n",
      ".. dropdown:: Usage\n",
      "\n",
      "  The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "  fetching / caching functions that downloads the data archive from\n",
      "  the original `20 newsgroups website <http://people.csail.mit.edu/jrennie/20Newsgroups/>`__,\n",
      "  extracts the archive contents\n",
      "  in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      "  :func:`sklearn.datasets.load_files` on either the training or\n",
      "  testing set folder, or both of them::\n",
      "\n",
      "    >>> from sklearn.datasets import fetch_20newsgroups\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "    >>> from pprint import pprint\n",
      "    >>> pprint(list(newsgroups_train.target_names))\n",
      "    ['alt.atheism',\n",
      "     'comp.graphics',\n",
      "     'comp.os.ms-windows.misc',\n",
      "     'comp.sys.ibm.pc.hardware',\n",
      "     'comp.sys.mac.hardware',\n",
      "     'comp.windows.x',\n",
      "     'misc.forsale',\n",
      "     'rec.autos',\n",
      "     'rec.motorcycles',\n",
      "     'rec.sport.baseball',\n",
      "     'rec.sport.hockey',\n",
      "     'sci.crypt',\n",
      "     'sci.electronics',\n",
      "     'sci.med',\n",
      "     'sci.space',\n",
      "     'soc.religion.christian',\n",
      "     'talk.politics.guns',\n",
      "     'talk.politics.mideast',\n",
      "     'talk.politics.misc',\n",
      "     'talk.religion.misc']\n",
      "\n",
      "  The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "  attribute is the integer index of the category::\n",
      "\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "  It is possible to load only a sub-selection of the categories by passing the\n",
      "  list of the categories to load to the\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "    >>> cats = ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "    >>> list(newsgroups_train.target_names)\n",
      "    ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      ".. dropdown:: Converting text to vectors\n",
      "\n",
      "  In order to feed predictive or clustering models with the text data,\n",
      "  one first need to turn the text into vectors of numerical values suitable\n",
      "  for statistical analysis. This can be achieved with the utilities of the\n",
      "  ``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "  example that extract `TF-IDF <https://en.wikipedia.org/wiki/Tf-idf>`__ vectors\n",
      "  of unigram tokens from a subset of 20news::\n",
      "\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "    ...               'comp.graphics', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> vectors.shape\n",
      "    (2034, 34118)\n",
      "\n",
      "  The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "  components by sample in a more than 30000-dimensional space\n",
      "  (less than .5% non-zero features)::\n",
      "\n",
      "    >>> vectors.nnz / float(vectors.shape[0])\n",
      "    159.01327...\n",
      "\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
      "  returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. dropdown:: Filtering text for more realistic training\n",
      "\n",
      "  It is easy for a classifier to overfit on particular things that appear in the\n",
      "  20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "  high F-scores, but their results would not generalize to other documents that\n",
      "  aren't from this window of time.\n",
      "\n",
      "  For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "  which is fast to train and achieves a decent F-score::\n",
      "\n",
      "    >>> from sklearn.naive_bayes import MultinomialNB\n",
      "    >>> from sklearn import metrics\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.88213...\n",
      "\n",
      "  (The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "  the training and test data, instead of segmenting by time, and in that case\n",
      "  multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "  yet of what's going on inside this classifier?)\n",
      "\n",
      "  Let's take a look at what the most informative features are:\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> def show_top10(classifier, vectorizer, categories):\n",
      "    ...     feature_names = vectorizer.get_feature_names_out()\n",
      "    ...     for i, category in enumerate(categories):\n",
      "    ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "    ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "    ...\n",
      "    >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "    alt.atheism: edu it and in you that is of to the\n",
      "    comp.graphics: edu in graphics it is for and of to the\n",
      "    sci.space: edu it that is in and space to of the\n",
      "    talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "  You can now see many things that these features have overfit to:\n",
      "\n",
      "  - Almost every group is distinguished by whether headers such as\n",
      "    ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "  - Another significant feature involves whether the sender is affiliated with\n",
      "    a university, as indicated either by their headers or their signature.\n",
      "  - The word \"article\" is a significant feature, based on how often people quote\n",
      "    previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "    wrote:\"\n",
      "  - Other features match the names and e-mail addresses of particular people who\n",
      "    were posting at the time.\n",
      "\n",
      "  With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "  barely have to identify topics from text at all, and they all perform at the\n",
      "  same high level.\n",
      "\n",
      "  For this reason, the functions that load 20 Newsgroups data provide a\n",
      "  parameter called **remove**, telling it what kinds of information to strip out\n",
      "  of each file. **remove** should be a tuple containing any subset of\n",
      "  ``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "  blocks, and quotation blocks respectively.\n",
      "\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "    0.77310...\n",
      "\n",
      "  This classifier lost over a lot of its F-score, just because we removed\n",
      "  metadata that has little to do with topic classification.\n",
      "  It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.76995...\n",
      "\n",
      "  Some other classifiers cope better with this harder version of the task. Try the\n",
      "  :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "  example with and without the `remove` option to compare the results.\n",
      "\n",
      ".. topic:: Data Considerations\n",
      "\n",
      "  The Cleveland Indians is a major league baseball team based in Cleveland,\n",
      "  Ohio, USA. In December 2020, it was reported that \"After several months of\n",
      "  discussion sparked by the death of George Floyd and a national reckoning over\n",
      "  race and colonialism, the Cleveland Indians have decided to change their\n",
      "  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
      "  its informal nickname -- the Tribe -- its new team name.\" \"It's not going to\n",
      "  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
      "  American-themed name.\"\n",
      "\n",
      "  https://www.mlb.com/news/cleveland-indians-team-name-change\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  - When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "    should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
      "    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "    lower because it is more realistic.\n",
      "  - This text dataset contains data which may be inappropriate for certain NLP\n",
      "    applications. An example is listed in the \"Data Considerations\" section\n",
      "    above. The challenge with using current text datasets in NLP for tasks such\n",
      "    as sentence completion, clustering, and other applications is that text\n",
      "    that is culturally biased and inflammatory will propagate biases. This\n",
      "    should be taken into consideration when using the dataset, reviewing the\n",
      "    output, and the bias should be documented.\n",
      "\n",
      ".. rubric:: Examples\n",
      "\n",
      "* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793567dd-8e91-48b7-bbb7-a0b164ef2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(newsgroups.data)\n",
    "targets = list(newsgroups.target)\n",
    "classes = list(newsgroups.target_names)\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "565f832c-b509-4484-b634-39aa8f009933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...      7\n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...      4\n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...      4\n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...      1\n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...     14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv into pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data={\"text\":data, \"label\":targets})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e1ae7a-d389-4662-9398-0bc156088548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216188f-9606-4737-b74a-68b2c4ba5b98",
   "metadata": {},
   "source": [
    "## 2. Load pretrained BERT from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90dca728-aca4-4d74-b170-8a73d47eb6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "model_id = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id) #, do_lower_case=True) #BertTokenizerFast?\n",
    "model = BertForSequenceClassification.from_pretrained(model_id, num_labels=num_classes, \n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11047d-8284-4541-893f-e485d5cb4a63",
   "metadata": {},
   "source": [
    "## 3. Train BERT for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68cb1d4e-c69d-4c3a-ad5f-4d76f5d11adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "501f399c-85cb-463a-b7ab-3e4d1c60c215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Tokenization - this may take a while: 5 - 10 seconds\n",
    "encoded_data = tokenizer.batch_encode_plus(df.text.values, \n",
    "                                           add_special_tokens=True, \n",
    "                                           return_attention_mask=True,\n",
    "                                           #pad_to_max_length=True,\n",
    "                                           padding='max_length',\n",
    "                                           truncation=True,\n",
    "                                           max_length=256,\n",
    "                                           return_tensors=\"pt\")\n",
    "input_ids = encoded_data[\"input_ids\"]\n",
    "attention_masks = encoded_data[\"attention_mask\"]\n",
    "\n",
    "labels = torch.tensor(df.label.values) #dtype=torch.long\n",
    "labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59d35946-b818-49b2-9a1d-5bfc7c8bcbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6357dbd-31d7-4af7-a512-221f12f9fdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2013,  1024,  3393,  2099,  2595,  3367,  1030, 11333,  2213,\n",
       "         1012,  8529,  2094,  1012,  3968,  2226,  1006,  2073,  1005,  1055,\n",
       "         2026,  2518,  1007,  3395,  1024,  2054,  2482,  2003,  2023,   999,\n",
       "         1029,  1050,  3372,  2361,  1011, 14739,  1011,  3677,  1024, 10958,\n",
       "         2278,  2509,  1012, 11333,  2213,  1012,  8529,  2094,  1012,  3968,\n",
       "         2226,  3029,  1024,  2118,  1997,  5374,  1010,  2267,  2380,  3210,\n",
       "         1024,  2321,  1045,  2001,  6603,  2065,  3087,  2041,  2045,  2071,\n",
       "         4372,  7138,  2368,  2033,  2006,  2023,  2482,  1045,  2387,  1996,\n",
       "         2060,  2154,  1012,  2009,  2001,  1037,  1016,  1011,  2341,  2998,\n",
       "         2482,  1010,  2246,  2000,  2022,  2013,  1996,  2397, 20341,  1013,\n",
       "         2220, 17549,  1012,  2009,  2001,  2170,  1037,  5318,  4115,  1012,\n",
       "         1996,  4303,  2020,  2428,  2235,  1012,  1999,  2804,  1010,  1996,\n",
       "         2392, 21519,  2001,  3584,  2013,  1996,  2717,  1997,  1996,  2303,\n",
       "         1012,  2023,  2003,  2035,  1045,  2113,  1012,  2065,  3087,  2064,\n",
       "         2425,  4168,  1037,  2944,  2171,  1010,  3194, 28699,  2015,  1010,\n",
       "         2086,  1997,  2537,  1010,  2073,  2023,  2482,  2003,  2081,  1010,\n",
       "         2381,  1010,  2030,  3649, 18558,  2017,  2031,  2006,  2023, 24151,\n",
       "         2559,  2482,  1010,  3531,  1041,  1011,  5653,  1012,  4283,  1010,\n",
       "         1011,  6335,  1011,  1011,  1011,  1011,  2716,  2000,  2017,  2011,\n",
       "         2115,  5101,  3393,  2099,  2595,  3367,  1011,  1011,  1011,  1011,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9c8e5b6-edad-4658-9e91-e67fa808dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training Parameters\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# no loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79ccfad4-eda6-47ff-aedc-0c779a466c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 100 Training loss: 0.08699866659939288\n",
      "Epoch 0 200 Training loss: 0.0741283438168466\n",
      "Epoch 0 300 Training loss: 0.06370054557298621\n",
      "Epoch 0 Training loss: 0.059356899381168574\n",
      "Epoch 1 100 Training loss: 0.029636690225452185\n",
      "Epoch 1 200 Training loss: 0.02620628282893449\n",
      "Epoch 1 300 Training loss: 0.023800453391547006\n",
      "Epoch 1 Training loss: 0.022785322856245602\n",
      "Epoch 2 100 Training loss: 0.015473079290241004\n",
      "Epoch 2 200 Training loss: 0.01399453843710944\n",
      "Epoch 2 300 Training loss: 0.012954272259958089\n",
      "Epoch 2 Training loss: 0.012418476688539432\n",
      "Epoch 3 100 Training loss: 0.008922530626878143\n",
      "Epoch 3 200 Training loss: 0.008337276668753476\n",
      "Epoch 3 300 Training loss: 0.007830981435254215\n",
      "Epoch 3 Training loss: 0.007551219072657169\n",
      "Epoch 4 100 Training loss: 0.005645932378247381\n",
      "Epoch 4 200 Training loss: 0.005474376626079902\n",
      "Epoch 4 300 Training loss: 0.005101131113090863\n",
      "Epoch 4 Training loss: 0.00492341618932914\n"
     ]
    }
   ],
   "source": [
    "#todo load model to device\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    j = 0\n",
    "    for i in range(0, input_ids.size(0), batch_size): # start - stop - step\n",
    "        j += 1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids[i:i+batch_size].to(device), \n",
    "                        attention_mask=attention_masks[i:i+batch_size].to(device),\n",
    "                        labels=labels[i:i+batch_size].to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "        if j%100 == 0:\n",
    "            print(f'Epoch {epoch} {j} Training loss: {total_loss/input_ids[0:i+batch_size].size(0)}')\n",
    "        \n",
    "    \n",
    "    print(f'Epoch {epoch} Training loss: {total_loss/input_ids.size(0)}')\n",
    "    # save the model\n",
    "    torch.save(model, f\"bert_{epoch}.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d6b8621-3a11-4d33-9ea0-606763cd2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), f\" bert_{epoch}.pt\")\n",
    "#model = BertForSequenceClassification.from_pretrained(model_id, num_labels=num_classes, \n",
    "#                                                      output_attentions=False,\n",
    "#                                                      output_hidden_states=False)\n",
    "#model.load_state_dict(torch.load(f\" bert_{epoch}.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "156a4776-bf06-410b-994f-009a36fe04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(f\"/Users/done/Documents/Hochschule/WS 2024/WI-Intelligente-Informationssysteme/archive/bert_4.bin\", weights_only=False)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7579d386-eb6b-4208-84ed-1b487bc586bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe97227-5fb3-42c1-abae-584f9fae0486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 11314 : 0.971\n",
      "1000 / 11314 : 0.976\n",
      "2000 / 11314 : 0.9746666666666667\n",
      "3000 / 11314 : 0.9715\n",
      "4000 / 11314 : 0.9716\n",
      "5000 / 11314 : 0.9721666666666666\n",
      "6000 / 11314 : 0.9727142857142858\n",
      "7000 / 11314 : 0.97275\n",
      "8000 / 11314 : 0.9741111111111111\n",
      "9000 / 11314 : 0.9754\n"
     ]
    }
   ],
   "source": [
    "#### Evaluate the model\n",
    "accuracy = 0\n",
    "batch_size = 1000\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, input_ids.size(0), batch_size): # start - stop - step\n",
    "        #outputs = model(input_ids=input_ids.to(device), attention_mask=attention_masks.to(device))\n",
    "        outputs = model(input_ids[i:i+batch_size].to(device), \n",
    "                        attention_mask=attention_masks[i:i+batch_size].to(device))\n",
    "        predictions = torch.argmax(outputs[0], dim=1).flatten()\n",
    "        accuracy += torch.sum(predictions == labels[i:i+batch_size].to(device)).item()\n",
    "        print(i,\"/\",input_ids.size(0),\":\",accuracy/labels[0:i+batch_size].size(0))\n",
    "print(\"Accuracy:\", accuracy/input_ids.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9bcca-d97e-466d-b866-cbe246747b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33491cec-c31d-4dd4-9d42-ec379cb0f2d4",
   "metadata": {},
   "source": [
    "# Second Try\n",
    "# \n",
    "# RESULT - not working well \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2de0ca2-3236-46e9-9061-a610ea88e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(data_home=\".\")\n",
    "data = list(newsgroups.data)\n",
    "targets = list(newsgroups.target)\n",
    "classes = list(newsgroups.target_names)\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669529d8-dea4-4dcd-bed6-c70840412244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Build a pytorch custom Dataset\n",
    "# A custom Dataset class must have these three methods: __init__ , __getitem__ , __len__\n",
    "# see: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_id = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id) #BertTokenizer\n",
    "model = BertModel.from_pretrained(model_id)         #BertModel\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    encoded_inputs = tokenizer(text, \n",
    "                               add_special_tokens=True, \n",
    "                               return_attention_mask=True,\n",
    "                               padding=\"max_length\", \n",
    "                               truncation=True, \n",
    "                               max_length=256, \n",
    "                               return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_inputs)\n",
    "    \n",
    "    return torch.sum(outputs.last_hidden_state, 1) # torch.Size([1, 256, 768]) [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "\n",
    "class NewsgroupDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data: list, labels: list, classes: list, preprocess=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.classes = classes\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.preprocess:\n",
    "            return preprocess(text), int(label)\n",
    "        else:\n",
    "            return text, int(label)\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.classes)\n",
    "        \n",
    "dataset = NewsgroupDataset(data=data, labels=targets, classes=classes, preprocess=preprocess)\n",
    "\n",
    "print(len(dataset))\n",
    "x,y = dataset[0]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2299f23-3ee9-4f0a-8ef8-8dbfb78bdece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and train\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e330b53-63fc-4c15-8802-a3586ebddb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53b7ed-7de2-4fcb-a609-c69b37669a97",
   "metadata": {},
   "source": [
    "## 3. Define a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e916e07-14c3-40d7-8559-69905f4f245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=20, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "device = torch.device(\"mps\")\n",
    "classifier = nn.Linear(768, dataset.num_classes())\n",
    "classifier.to(device)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ebca6-f001-42eb-8d79-ee978b17f74e",
   "metadata": {},
   "source": [
    "## 4. Train only the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f511409e-7849-45c6-ab5b-36256974fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=2e-5) \n",
    "#see: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7da90bc6-f456-4547-ba73-21816bd7f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train(classifier, optimizer, train_loader, loss_function, num_classes):\n",
    "    \n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "    j, n = 0, 0\n",
    "    for batch in train_loader:\n",
    "        j += 1\n",
    "        optimizer.zero_grad()\n",
    "        embeddings, labels = batch\n",
    "        n += len(labels)\n",
    "        outputs = classifier(embeddings.to(device))\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        targets = np.array([labels]).reshape(-1)\n",
    "        one_hot_targets = torch.tensor(np.eye(num_classes)[targets], dtype=torch.float32).to(device)\n",
    "        loss = loss_function(outputs, one_hot_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(f'Training loss: {total_loss/n}')\n",
    "    print(f'Training loss: {total_loss/len(train_loader)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03e530b7-f40b-43ce-9337-7be3d165abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(classifier, test_loader, loss_function):\n",
    "    classifier.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    j = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            embeddings, labels = batch\n",
    "            j += len(labels)\n",
    "            outputs = classifier(embeddings.to(device))\n",
    "            outputs = torch.squeeze(outputs)\n",
    "            targets = np.array([labels]).reshape(-1)\n",
    "            one_hot_targets = torch.tensor(np.eye(num_classes)[targets], dtype=torch.float32).to(device)\n",
    "            loss = loss_function(outputs, one_hot_targets)            \n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            total_acc += (predictions == labels.to(device)).sum().item()\n",
    "            print(f'{j} of {len(test_dataset)}  Test loss: {total_loss/j} Test acc: {total_acc/j*100}%')\n",
    "    print(f'Test loss: {total_loss/len(test_dataset)} Test acc: {total_acc/len(test_dataset)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b27364a3-991d-4551-8187-7c6e670c4985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### EPOCH 0 #######\n",
      "64 of 3395  Test loss: 0.42303887009620667 Test acc: 9.375%\n",
      "128 of 3395  Test loss: 0.4134484529495239 Test acc: 10.15625%\n",
      "192 of 3395  Test loss: 0.4286746084690094 Test acc: 7.8125%\n",
      "256 of 3395  Test loss: 0.4171008840203285 Test acc: 8.59375%\n",
      "320 of 3395  Test loss: 0.41969203352928164 Test acc: 7.187499999999999%\n",
      "384 of 3395  Test loss: 0.4212421327829361 Test acc: 6.25%\n",
      "448 of 3395  Test loss: 0.4302159547805786 Test acc: 5.580357142857143%\n",
      "512 of 3395  Test loss: 0.42967483401298523 Test acc: 5.859375%\n",
      "576 of 3395  Test loss: 0.42965347237057155 Test acc: 5.729166666666666%\n",
      "640 of 3395  Test loss: 0.42942704558372496 Test acc: 5.9375%\n",
      "704 of 3395  Test loss: 0.4317957813089544 Test acc: 6.392045454545454%\n",
      "768 of 3395  Test loss: 0.43528714776039124 Test acc: 5.859375%\n",
      "832 of 3395  Test loss: 0.4307731023201576 Test acc: 6.25%\n",
      "896 of 3395  Test loss: 0.4327816388436726 Test acc: 6.138392857142857%\n",
      "960 of 3395  Test loss: 0.4341835121313731 Test acc: 5.9375%\n",
      "1024 of 3395  Test loss: 0.4349407497793436 Test acc: 5.859375%\n",
      "1088 of 3395  Test loss: 0.4321652528117685 Test acc: 6.158088235294118%\n",
      "1152 of 3395  Test loss: 0.4312376429637273 Test acc: 6.076388888888888%\n",
      "1216 of 3395  Test loss: 0.4296826720237732 Test acc: 6.003289473684211%\n",
      "1280 of 3395  Test loss: 0.43278867453336717 Test acc: 5.9375%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m####### EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m #######\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#train(classifier, optimizer, train_loader, loss_function, num_classes=dataset.num_classes())\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(classifier, test_loader, loss_function)\u001b[0m\n\u001b[1;32m      5\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[13], line 44\u001b[0m, in \u001b[0;36mNewsgroupDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     42\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mint\u001b[39m(label)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text, \u001b[38;5;28mint\u001b[39m(label)\n",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, \n\u001b[1;32m     16\u001b[0m                            add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     17\u001b[0m                            return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m                            max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m     21\u001b[0m                            return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:409\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states))\n\u001b[0;32m--> 409\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n\u001b[1;32m    411\u001b[0m         key_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/block2_sft/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    print(f\"####### EPOCH {epoch} #######\")\n",
    "    train(classifier, optimizer, train_loader, loss_function, num_classes=dataset.num_classes())\n",
    "    evaluate(classifier, test_loader, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb04c687-d5cd-4235-ac87-384b44502511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Backup #############\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "model_id = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id) #, do_lower_case=True) #BertTokenizerFast?\n",
    "model = BertModel.from_pretrained(model_id, output_hidden_states=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b9e63-4765-4cea-a008-98108ee5204d",
   "metadata": {},
   "source": [
    "# Working with datasets\n",
    "\n",
    "## PyTorch Datasets\n",
    "\n",
    "Build a pytorch custom dataset based on the Dataset class. A custom Dataset class must implement these three methods: __init__ , __getitem__ , __len__\n",
    "\n",
    "see: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "see: https://rumn.medium.com/how-to-create-a-custom-pytorch-dataset-with-a-csv-file-e64b89bc2dcc\n",
    "\n",
    "## Transformer Datasets\n",
    "\n",
    "Use the datasets library from hugging face: pip install datasets\n",
    "\n",
    "see: https://huggingface.co/docs/datasets/index\n",
    "\n",
    "```\n",
    "from datasets import load_dataset  \n",
    "my_dataset = load_dataset(\"csv\", data_files=\"data.csv\")\n",
    "split = my_dataset['train'].train_test_split(test_size=0.3, seed=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98ec7a-fb58-4cb5-b349-4360f9bbf147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://medium.com/@lokaregns/fine-tuning-transformers-with-custom-dataset-classification-task-f261579ae068"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
