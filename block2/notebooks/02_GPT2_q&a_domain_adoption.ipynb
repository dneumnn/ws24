{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1992378a-474a-4fd5-8278-0e460dc91a33",
   "metadata": {},
   "source": [
    "# Question Answering domain specific Fine-Tuning\n",
    "\n",
    "Goal: Train a chatbot model that responds specifically within your domain\n",
    "\n",
    "see https://medium.com/@rupaak/how-to-fine-tune-gpt-2-for-a-domain-specific-chatbot-46e9ca64bc86\n",
    "\n",
    "Supervised Fine-Tuning of GPT using Huggingface Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b57ceb-ba85-4e21-8b54-a3847c03700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "#from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8fabc-e74f-4f49-a92f-b1c873d6fbb7",
   "metadata": {},
   "source": [
    "## Loading the Squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55ddb22-8db1-4524-984d-96ee75246e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b15b1e-6add-48b8-a802-c356d3c965a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build custom Dataset from loaded squad dataset\n",
    "path = \"./data.jsonl\"\n",
    "\n",
    "def _build_dataset(dataset):\n",
    "    custom_dataset = []\n",
    "    for item in dataset['train']:\n",
    "        item['train'] = True\n",
    "        custom_dataset.append(json.dumps(item))\n",
    "    \n",
    "    for item in dataset['validation']:\n",
    "        item['train'] = False\n",
    "        custom_dataset.append(json.dumps(item))\n",
    "    \n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(custom_dataset))\n",
    "\n",
    "_build_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895962fa-f0ff-4e77-aa5d-72eb0d28a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load custom Dataset\n",
    "path = \"./data.jsonl\"\n",
    "\n",
    "def _load_dataset(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    data = [json.loads(line) for line in lines]\n",
    "    \n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        question = item['question']\n",
    "        context = item['context']\n",
    "        answers = item['answers']\n",
    "\n",
    "        input_text = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        formatted_data.append({\n",
    "            'input_text': input_text,\n",
    "            'target_text': input_text, #str(context), #answers['text'][0] \n",
    "        })\n",
    "    \n",
    "    return Dataset.from_dict({'text': [item['input_text'] for item in formatted_data],\n",
    "                              'labels': [item['target_text'] for item in formatted_data]})\n",
    "\n",
    "\n",
    "dataset = _load_dataset(path)  # Our data file name is 'data.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d19981-206e-4e39-9f4f-f7aed675bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2db15b-e1e5-4bfd-bdf8-d557edf4e7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c273928a7ed94534947a870a30e27093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Tokenize the dataset\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "#def tokenize_function(examples):\n",
    "#    #return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
    "#    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def tokenize(examples):\n",
    "    result = tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(examples['labels'], padding=\"max_length\", truncation=True)\n",
    "    result['labels'] = labels['input_ids']\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(lambda example: tokenize(example), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aed3e6c-9857-407b-86ad-82e1d9842e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 68718\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 29451\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted = tokenized_datasets.train_test_split(test_size=0.3, shuffle=True, seed=42)\n",
    "splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f49e765-e72a-4c0e-8668-d21991e24e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use a Data Collator for preparing a Bach\n",
    "## see https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a77415b-7b27-4f0c-8cc2-1a7c67e24026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e1bf194-3ed0-47c0-9e08-96924ebc6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Huggingface Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    #per_device_train_batch_size=4,\n",
    "    #per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1, #3\n",
    "    weight_decay=0.01,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    #data_collator=data_collator,\n",
    "    train_dataset=splitted['train'],\n",
    "    eval_dataset=splitted['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90a7bf32-824a-456f-86f7-54bad0848440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'labels', 'input_ids', 'attention_mask'])\n",
      "text  :         Question: What was t ...\n",
      "labels:         [24361, 25, 1867, 373, 262, 1438, 286, 262, 1628, 5495] ...\n",
      "input_ids:      [24361, 25, 1867, 373, 262, 1438, 286, 262, 1628, 5495] ...\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ...\n"
     ]
    }
   ],
   "source": [
    "for item in splitted['train']:\n",
    "    print(item.keys())\n",
    "    print(\"text  :        \", item['text'][0:20],\"...\")\n",
    "    print(\"labels:        \", item['labels'][0:10],\"...\")\n",
    "    print(\"input_ids:     \", item['input_ids'][0:10],\"...\")\n",
    "    print(\"attention_mask:\", item['attention_mask'][0:10],\"...\")\n",
    "    break\n",
    "\n",
    "# label, text, input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5172dd5-4e92-48e7-9a6f-4e3b1be3202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee7927-e0eb-43fa-bc61-13f6d480ca93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
