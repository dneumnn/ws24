{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1992378a-474a-4fd5-8278-0e460dc91a33",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of GPT using Huggingface Tools\n",
    "\n",
    "## Domain Adaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedd1057-2674-4365-b72a-1b4794b577a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2fb0a-1154-4519-aa19-10c3aaa41c7f",
   "metadata": {},
   "source": [
    "## 1. Prepare and load traing and evaluation data\n",
    "\n",
    "Use the 'context' in data.jsonl for domain adaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74f9d53-1ee5-468a-b1e6-b7fc9ede1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./data.jsonl\"\n",
    "train_output_file = \"./train_data.txt\"\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(train_output_file, \"w\") as outfile:\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        if data['train']:\n",
    "            context = data['context']\n",
    "            context = context.strip()\n",
    "            outfile.write(context)\n",
    "\n",
    "eval_output_file = \"./eval_data.txt\"\n",
    "with open(eval_output_file, \"w\") as outfile:\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        if not data['train']:\n",
    "            context = data['context']\n",
    "            context = context.strip()\n",
    "            outfile.write(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68d8a034-9b24-4c31-80be-83cca7d5a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the pre_trained GPT2 model\n",
    "         # \"gpt2\"       #: 124 million parameters \n",
    "model_id = \"gpt2-medium\"#: 345 million parameters \n",
    "         # \"gpt2-Large\" #: 774 million parameters\n",
    "         # \"gpt2-xl\"    #: 1.5 billion parameters \n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id, clean_up_tokenization_spaces=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66b42553-7c26-4a92-abe5-bdb749484adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token  \n",
    "# or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c73e3941-8604-444a-80b4-f6aa1ded3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Build a custom PyTorch Dataset\n",
    "class CustomDataset(Dataset): # from torch.utils.data\n",
    "    def __init__(self, tokenizer, file_path, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(file_path, \"r\") as f:\n",
    "            self.text = f.read().splitlines()\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_inputs = self.tokenizer(self.text[idx], \n",
    "                                          truncation=True, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.block_size, \n",
    "                                          return_tensors=\"pt\")\n",
    "        \n",
    "        tokenized_inputs['labels'] = tokenized_inputs['input_ids']\n",
    "        return tokenized_inputs\n",
    "        \n",
    "\n",
    "train_dataset = CustomDataset(tokenizer, train_output_file, 128)\n",
    "#eval_dataset = CustomDataset(tokenizer, eval_output_file, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7aa7584-4d49-4b16-a36c-6b85f7767dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usinig devivs: mps\n"
     ]
    }
   ],
   "source": [
    "# set device to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Usinig devivs:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c69b50bc-ac0b-491d-abdb-93772a25e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a data collator that dynamically pad the sequences \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cfcb101-0b1e-45d4-8fad-9bfeac767a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Huggingface Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    #per_device_train_batch_size=2,\n",
    "    output_dir='./results_2',\n",
    "    logging_dir='./logs_2',\n",
    "    num_train_epochs=3, #large dataset -> small num epochs, small dataset --> more epochs\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,\n",
    "    eval_strategy=\"no\",    \n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    #per_device_eval_batch_size=4,\n",
    "    #weight_decay=0.01,\n",
    "    #save_steps=10_000,\n",
    "    #save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4389afb4-8ce7-441f-a21c-99d8915cc1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 04:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.565700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.522600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.214900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.301400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.7042135095596314, metrics={'train_runtime': 251.5318, 'train_samples_per_second': 4.735, 'train_steps_per_second': 0.596, 'total_flos': 276520631795712.0, 'train_loss': 0.7042135095596314, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "420c013d-af39-442f-a820-fce93d062f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the 'Super Bowl 50'?\n",
      "\n",
      "The 50th Super Bowl will be played on Sunday, February 5, 2016 at the Mercedes-Benz Superdome in New Orleans, Louisiana. The game will be broadcast live on NBC, ABC, FOX, CBS, ESPN and the NFL Network. The game will be broadcast live online on NBC Sports Live Extra starting at 11:00 p.m. ET on Sunday, February 5, 2016.\n",
      "\n",
      "What is the Super Bowl 50 broadcast schedule\n"
     ]
    }
   ],
   "source": [
    "orig_model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "prompt = \"What is the 'Super Bowl 50'?\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "\n",
    "orig_model.eval()\n",
    "orig_model.to(device)\n",
    "output = orig_model.generate(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        max_length= 100,\n",
    "                        num_beams=5,\n",
    "                        temperature=1.5,\n",
    "                        top_k=50,\n",
    "                        do_sample=True)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c9ec83ec-f5d4-4a49-9cf5-8f5b9f8486ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the 'Super Bowl 50'?' It is the 50th anniversary of the Super Bowl XLI between the Atlanta Falcons and New England Patriots. The game was played on December 12, 2000 at Mercedes-Benz Stadium in Atlanta, Georgia. The home team won 27-14. The victory was the most lopsided in Super Bowl history with the score being 27-14 in favor of the home team. The victory also marked the beginning of the end for the infamous \"Deflategate\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the 'Super Bowl 50'?\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "output = model.generate(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        max_length= 100,\n",
    "                        num_beams=5,\n",
    "                        temperature=1.3,\n",
    "                        top_k=50,\n",
    "                        do_sample=True)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5526449-12e0-4973-8937-83620516a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Backup ################\n",
    "# integrate model into chatbot\n",
    "# see https://medium.com/@rupaak/use-fine-tuned-gpt-2-for-chatbot-5fdf4908fbca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15ee7927-e0eb-43fa-bc61-13f6d480ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./data.jsonl\"\n",
    "j = 0\n",
    "with open(input_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        context = data['context']\n",
    "        if \"Super Bowl 50\" in context:\n",
    "            j += 1\n",
    "            print(f\"----- {j} ------\")\n",
    "            print(context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe32d669-6d62-4943-b6fb-b570da0c2d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
