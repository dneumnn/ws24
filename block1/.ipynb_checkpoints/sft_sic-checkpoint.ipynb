{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c13217d-51d1-4e69-8082-82dd00e678d9",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning\n",
    "\n",
    "Fine-tune GPT2 for Question Answering Task\n",
    "\n",
    "see: https://www.kaggle.com/code/dsmeena/pytorch-fine-tuning-gpt2-for-questionanswering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3f2c6a-a4eb-4b97-8c46-db38ec49c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets, torch, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbfde48-a1b6-4ccd-af50-86a6812e3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00a664-84a1-41d3-9561-3e2d9f6b9e54",
   "metadata": {},
   "source": [
    "## Load the Quetsion & Answer squad Dataset from Pytorch\n",
    "\n",
    "https://pytorch.org/text/stable/datasets.html#squad-1-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c71a0a1-a63f-4fc9-9b44-aa899a59e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599\n",
      "10570\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"squad\")[\"train\"]\n",
    "print(len(train_dataset))\n",
    "test_dataset = load_dataset(\"squad\")[\"validation\"]\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7c241eb-46bd-465f-a2e2-9fc793a5715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'context', 'question', 'answers'])\n",
      "Q To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "A {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
      "Q What is in front of the Notre Dame Main Building?\n",
      "A {'text': ['a copper statue of Christ'], 'answer_start': [188]}\n",
      "Q The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      "A {'text': ['the Main Building'], 'answer_start': [279]}\n",
      "Q What is the Grotto at Notre Dame?\n",
      "A {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]}\n",
      "Q What sits on top of the Main Building at Notre Dame?\n",
      "A {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]}\n"
     ]
    }
   ],
   "source": [
    "# get some elements of the dataset\n",
    "generator = train_dataset.iter(batch_size=5)\n",
    "y = 0\n",
    "for x in generator:\n",
    "    print(x.keys())\n",
    "    questions = x['question']\n",
    "    answers = x['answers']\n",
    "    for q, a in zip(questions, answers):\n",
    "        print(\"Q\", q)\n",
    "        print(\"A\", a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dddf928e-8a37-46b9-9ec3-c90a36787fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be4db0acb8001400a502ec',\n",
       " 'title': 'Super_Bowl_50',\n",
       " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       "  'answer_start': [177, 177, 177]}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1623b8-fa17-4043-812f-50b60ad7af8d",
   "metadata": {},
   "source": [
    "## Build a custom dataset out of squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2a4476c-ed53-40ef-9297-97cda5f0040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, orig_dataset, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = orig_dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        answer = item['answers']['text'][0] # get the first answer from answers\n",
    "        \n",
    "        # do encoding of the context and question \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length',   \n",
    "            max_length=384,     #max prompt size of GPT2\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # get start and end positions of answer in input_ids\n",
    "        input_ids = encoding['input_ids']\n",
    "        answer_start = item['answers']['answer_start'][0]\n",
    "        answer_end = answer_start + len(answer)\n",
    "        \n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        for i, token_id in enumerate(input_ids):\n",
    "            if i == answer_start:\n",
    "                start_positions.append(i)\n",
    "            else:\n",
    "                start_positions.append(-100)\n",
    "            \n",
    "            if i == answer_end:\n",
    "                end_positions.append(i)\n",
    "            else:\n",
    "                end_positions.append(-100)\n",
    "        \n",
    "        # Create input tensors\n",
    "        inputs = {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(encoding['token_type_ids'], dtype=torch.long),\n",
    "            'start_positions': torch.tensor(start_positions, dtype=torch.float),  # start and end positions should be float\n",
    "            'end_positions': torch.tensor(end_positions, dtype=torch.float)\n",
    "        }\n",
    "        \n",
    "        return inputs, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0beb45b-d25e-4978-8c7a-3b2df94f6bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ac1b05f-09a1-49b3-8ec8-982300ee029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = SquadDataset(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66c67ba0-4184-4f4d-b912-2dd8809f2465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question & Context:\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Answer:\n",
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "len(train)\n",
    "inputs , answer = train[0]\n",
    "input_ids = inputs['input_ids']\n",
    "print(\"Question & Context:\")\n",
    "print(tokenizer.decode(input_ids))\n",
    "print(\"Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82cbf06e-7c72-41ee-a6dd-95f4c0d1d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torch DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the Dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    SquadDataset(train_dataset, tokenizer),\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    SquadDataset(test_dataset, tokenizer),\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e304c849-d512-463c-92bd-4ac6b567b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Our context is:\n",
      " tensor([[ 2061,   614,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 2061, 22987,  6903,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,  3858,   286,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2061,   466, 31877,  ..., 50256, 50256, 50256],\n",
      "        [ 2061,   547,   973,  ..., 50256, 50256, 50256],\n",
      "        [ 8496,  2073,  2098,  ..., 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Iterate through DataLoader\n",
    "\n",
    "for batch in train_dataloader:  \n",
    "    print(len(batch[0]['input_ids'])) #each batch contains 16 input_ids\n",
    "    print(f\"Our context is:\\n {batch[0]['input_ids']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a5218a4-3c5a-4c7e-a985-54fde4503e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dd02ab14-1df0-45ae-ba3f-9629182a8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Load GPT2\n",
    "#from transformers import AutoModel\n",
    "#model = AutoModel.from_pretrained(\"gpt2\").to(device)\n",
    "#print(model)\n",
    "#from transformers import AutoModelWithLMHead\n",
    "#model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07197e77-038c-47ab-813b-f075688e05db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40e76a18-9e87-4836-b42c-7efd15320f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForQuestionAnswering(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"gpt2\").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "91eecc34-8dc8-43cc-bb4a-4e0856496ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-5\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f286c82-798a-4d8d-aae8-95480faa651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "# loss_fn = model.get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455b3ee-7c93-4444-b70c-8bf12357dcef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba1b85-b160-4a91-a0b9-a6c948f5f0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517045af-4414-49e0-8e7a-7382f76b5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer):\n",
    "    \n",
    "    # set the model to training model\n",
    "    model.train()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # previous tokens\n",
    "        input_ids = batch[0]['input_ids'].to(device)\n",
    "        attention_mask = batch[0]['attention_mask'].to(device)\n",
    "        token_type_ids = batch[0]['token_type_ids'].to(device)\n",
    "        start_positions = batch[0]['start_positions'].to(device)\n",
    "        end_positions = batch[0]['end_positions'].to(device)\n",
    "        \n",
    "        labels = {\n",
    "            'start_positions': start_positions,\n",
    "            'end_positions': end_positions\n",
    "        }\n",
    "        \n",
    "       # get outputs from model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_start = nn.CrossEntropyLoss()(outputs.start_logits, start_positions)\n",
    "        loss_end = nn.CrossEntropyLoss()(outputs.end_logits, end_positions)\n",
    "        loss = (loss_start + loss_end) / 2  # average loss for start and end positions\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    # set the model of evaluation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # previous tokens\n",
    "            input_ids = batch[0]['input_ids'].to(device)\n",
    "            attention_mask = batch[0]['attention_mask'].to(device)\n",
    "            token_type_ids = batch[0]['token_type_ids'].to(device)\n",
    "            start_positions = batch[0]['start_positions'].to(device)\n",
    "            end_positions = batch[0]['end_positions'].to(device)\n",
    "\n",
    "            labels = {\n",
    "                'start_positions': start_positions,\n",
    "                'end_positions': end_positions\n",
    "            }\n",
    "\n",
    "           # get outputs from model\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "            # calculate loss\n",
    "            loss_start = nn.CrossEntropyLoss()(outputs.start_logits, start_positions)\n",
    "            loss_end = nn.CrossEntropyLoss()(outputs.end_logits, end_positions)\n",
    "            loss = (loss_start + loss_end) / 2  # average loss for start and end positions\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Print the validation loss for this epoch\n",
    "    print(f\"Validation Loss: {val_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b2a2d-cc2f-46c6-bee8-5b7cb08fbb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6e706-a1d3-48d7-bea4-8f4dbe4f185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n ---------------------------\")\n",
    "    train_loop(train_dataloader, model, optimizer)\n",
    "    test_loop(test_dataloader, model)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "# model returns a CausalLMOutputWithCrossAttentions object, not just a loss. We can get the loss using loss attribute on it.\n",
    "# This is the recommeded way of obtaining the loss value when using the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11b6d8-0cea-44e6-96ea-cb041f6f1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_QA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
