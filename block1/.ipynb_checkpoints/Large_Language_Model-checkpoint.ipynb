{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c312b6fd-8064-44bc-b644-5c98859fcfb6",
   "metadata": {},
   "source": [
    "# Large Langugae Model\n",
    "\n",
    "A language model is a probability distribution over sequences of a vocabulary.\n",
    "\n",
    "Let V be a vocabulary, items could be characters, tokens or words. A language model assigns a probability to each sequence of items. \n",
    "\n",
    "Let $(w_1, w_2, ... ,w_{n-1})$ be a sequence of words from vocabulary V and a LM a language model over V with probablity P then:\n",
    "\n",
    "$$ P(w_n|w_1, ... ,w_{n-1}) = neuralnetwork(w_1, ... ,w_{n-1})$$\n",
    "\n",
    "The model predicts the next word based on the history.\n",
    "\n",
    "We would like a model to assign higher probabilities to sentences that are real and syntactically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d8d42d-b200-4a7e-b1b8-0d2a72896394",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70452bd6-b0b1-4a10-a9b4-112c588f4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Load the large language model GPT-2 from transformers #############\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "877843c7-386e-44d7-8b4a-8accce6395a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Alice is a\"\n",
    "input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "loss, logits = outputs[:2]\n",
    "sentence_prob = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd8b60b3-0913-4163-9972-94f4bca8a423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.280515432357788"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc77621-4f30-4774-a297-d4bda9406286",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "input_ids = encoded_input.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32945e47-3d0e-4447-b505-7d20e04d3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #A transformers.modeling_outputs.CausalLMOutputWithCrossAttentions \n",
    "    #or a tuple of torch.FloatTensor (if return_dict=False is passed or when config.return_dict=False)\n",
    "    outputs = model.forward(input_ids=input_ids, output_hidden_states=True, return_dict=True, labels=input_ids)\n",
    "loss, logits = outputs[:2]\n",
    "sentence_prob = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2d5cfc2-4db1-42c6-acf0-4ebadf0952f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0421e-03, 3.0297e-03, 1.1638e-04,  ..., 8.3673e-07, 4.0477e-06,\n",
       "        1.9094e-03])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size))\n",
    "# â€” Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
    "from torch.nn import Softmax\n",
    "m = Softmax(dim=0)\n",
    "y = m(logits.flatten())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b32fe3c8-42e2-4377-a49b-ba4a0e1fbe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "t = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "m = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "encoded_text = t(text, return_tensors=\"pt\")\n",
    "\n",
    "#1. step to get the logits of the next token\n",
    "with torch.inference_mode():\n",
    "  outputs = m(**encoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bea93c52-90cf-4553-a0f0-5a94ab71ab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. step to get the logits of the next token\n",
      "<class 'NoneType'> <class 'torch.Tensor'>\n",
      "torch.Size([1, 1, 50257])\n",
      "torch.Size([50257])\n",
      "tensor([-34.4592, -34.0647, -37.3240,  ..., -42.2592, -40.6827, -34.5264])\n",
      "2. step to convert the logits to probabilities\n",
      "torch.Size([50257])\n",
      "tensor([2.0421e-03, 3.0297e-03, 1.1638e-04,  ..., 8.3673e-07, 4.0477e-06,\n",
      "        1.9094e-03])\n",
      "3. step to get the top 10 and put all together\n",
      "[(',', tensor(0.0643)), ('.', tensor(0.0447)), (' and', tensor(0.0286)), ('\\n', tensor(0.0277)), (\"'s\", tensor(0.0254)), (' to', tensor(0.0176)), ('-', tensor(0.0164)), (' is', tensor(0.0158)), (' in', tensor(0.0128)), (' of', tensor(0.0119))]\n",
      ", tensor(0.0643)\n"
     ]
    }
   ],
   "source": [
    "print(\"1. step to get the logits of the next token\")\n",
    "loss, logits = outputs.loss, outputs.logits\n",
    "print(type(loss), type(logits))\n",
    "print(logits.shape)\n",
    "next_token_logits = logits[0, -1, :] # batch_size, sequence_length, config.vocab_size)\n",
    "print(next_token_logits.shape)\n",
    "print(next_token_logits)\n",
    "\n",
    "print(\"2. step to convert the logits to probabilities\")\n",
    "next_token_probs = torch.softmax(next_token_logits, -1)\n",
    "print(next_token_probs.shape)\n",
    "print(next_token_probs)\n",
    "\n",
    "print(\"3. step to get the top 10 and put all together\")\n",
    "topk_next_tokens= torch.topk(next_token_probs, 10)\n",
    "topk_next_token_list = [(t.decode(idx), prob) for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values)] \n",
    "print(topk_next_token_list)\n",
    "\n",
    "# now sample on top 10 or get the first one\n",
    "next_token, next_prob = topk_next_token_list[0]\n",
    "\n",
    "print(next_token, next_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40a06f-9d8f-430e-9c00-33e67fc24e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Backup #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5016adf0-1843-46e3-af78-e8275b0dd8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "########## Let have a look at the vocabulary of GPT-2 ##########\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(type(vocab))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b94961-ad9c-4f89-82d8-b000fb5795cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44484]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[44484]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [tokenizer.encode(\"Alice\")]\n",
    "print(s)\n",
    "torch.tensor(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef71ef70-93b6-4026-8f83-9b39d163edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/eamkon/lib/python3.11/site-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Alice\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_outputs = model.generate(input_ids, do_sample=True, num_return_sequences=3, output_scores=True, max_new_tokens=10)\n",
    "\n",
    "# only use id's that were generated\n",
    "# gen_sequences has shape [3, 15]\n",
    "gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1]:]\n",
    "\n",
    "# let's stack the logits generated at each step to a tensor and transform\n",
    "# logits to probs\n",
    "probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
    "\n",
    "# now we need to collect the probability of the generated token\n",
    "# we need to add a dummy dim in the end to make gather work\n",
    "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
    "\n",
    "# now we can do all kinds of things with the probs\n",
    "\n",
    "# 1) the probs that exactly those sequences are generated again\n",
    "# those are normally going to be very small\n",
    "unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "\n",
    "# 2) normalize the probs over the three sequences\n",
    "normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
    "assert normed_gen_probs[:, 0].sum() == 1.0, \"probs should be normalized\"\n",
    "\n",
    "# 3) compare normalized probs to each other like in 1)\n",
    "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e243ae-818a-4182-9497-356e10eaea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
