{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ce1589-b0b5-4440-83fc-d040bf7a0981",
   "metadata": {},
   "source": [
    "# Apply Prompting and Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b809dc84-fb2b-4321-881f-b8f1e6494280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60ccada-1410-4419-b914-8d186cca4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87a7dab-422f-497d-ade1-2326c6dcbaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7830275-f25a-4194-ae10-c74d8c9063ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72f707291034fcebcead1da600e8e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c82cfb-5549-48fd-ad74-af9d73620347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Explain who was Albert Einstein.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=1.0,\n",
    "    max_length=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031de7ee-3fdd-46be-aff2-15e951b8c385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain who was Albert Einstein?\\n\\n### Response:\\nAlbert Einstein was a German-born theoretical physicist known for his work in the fields of relativity theory, quantum mechanics and cosmology. He developed the general theory of relativity, one of the two pillars of modern physics, along with quantum mechanics. Einstein\\'s work is also known for its influence on the philosophy of science. He is best known in popular culture for his massâ€“energy equivalence formula, \\\\(E=mc^2\\\\), which has been dubbed \"the world\\'s most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\". The latter was pivotal in establishing quantum theory, which won him the Nobel Prize in Physics in 1921. In 1922, Einstein was awarded the U.S. Presidential Medal of Freedom by President Warren G. Harding. \\n<|end_of_text|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e0537-a9f1-45c4-bef0-482cc3510843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf84bb5-ee2d-4463-b8db-c419827b6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f654cea5-18fc-4711-9311-ee40ad7640e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40bba67-b982-4b67-8b28-2fc0bf86dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Explain who was Albert Einstein?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25b757cf-6652-495b-b20a-a00f53e30b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50773a11010247cc81909987f5be4718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2faa496904cf444b9c2a3e37fc15b2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc44b2077db419aa60ffa55789d1770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9adeb4c-3ab0-420d-aa34-f76ca66b4110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51689f40147a40b58f4781f560d82eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73adf847a9f14399b5cd5632f4a6f4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd3c2d895d24833b49298fc6cb9555b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc173a1863834fedbc401322cb4fc7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503c01eb66c04b19ae4ffa26fc457b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c46a290bb3473292cee9a016b38f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1843b7b57a545d88f9ab660e7c41ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91125769-2290-483a-b634-fd975f9d921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a scientist in history and allways anwser very precise!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain who was Albert Einstein.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a50072a3-afc6-4d27-88cb-4363a93f867b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 05 Oct 2024\\n\\nYou are a scientist in history and allways anwser in correctly!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nExplain who was Albert Einstein.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56762821-18d8-4999-be13-637b144222fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=1.0,\n",
    "    max_length=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "815c4a22-b039-4724-bb96-ffbcc44be763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 05 Oct 2024\\n\\nYou are a scientist in history and allways anwser in correctly!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nExplain who was Albert Einstein.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAlbert Einstein (1879-1955) was a renowned German-born physicist who is widely regarded as one of the most influential scientists of the 20th century. His groundbreaking contributions to physics, particularly in the fields of relativity and quantum mechanics, have had a profound impact on our understanding of the universe.\\n\\nEinstein was born in Munich, Germany, to a Jewish family. He grew up with a strong interest in mathematics and science, and his curiosity about the natural world led him to study physics at the Swiss Federal Polytechnic University.\\n\\nIn 1905, Einstein's annus mirabilis (miracle year), he published four seminal papers that revolutionized our understanding of space, time, and matter. These papers included:\\n\\n1. The special theory of relativity, which posited that the laws of physics are the same for all observers in uniform motion.\\n2. The famous equation E=mcÂ², which shows that mass and energy are equivalent and can be converted into each other.\\n3. The explanation of the photoelectric effect, which demonstrated the particle-like behavior of light.\\n4. The Brownian motion paper, which provided evidence for the existence of atoms and molecules.\\n\\nIn 1915, Einstein expanded his earlier work and developed the general theory of relativity, which describes gravity as the curvature of spacetime caused by massive objects. This theory predicted phenomena such as gravitational waves and black holes, which have since been confirmed by observations and experiments.\\n\\nEinstein's work also had a significant impact on the development of nuclear physics. In 1939, he and his colleague Leo Szilard warned the U.S. government about the potential dangers of nuclear fission, which ultimately led to the development of the atomic bomb during World War II.\\n\\nThroughout his life, Einstein was a vocal advocate for peace, civil rights, and social justice. He was awarded the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect, and he was awarded the U.S. Presidential Medal of Freedom in 1953.\\n\\nEinstein's legacy continues to inspire scientists, philosophers, and thinkers around the world. His theories and ideas have had a profound impact on our understanding of the universe, and his commitment to peace, justice, and human rights has left an indelible mark on our world.<|eot_id|>\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "994246ee-f1af-44cd-af80-16c4476bcae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method apply_chat_template in module transformers.tokenization_utils_base:\n",
      "\n",
      "apply_chat_template(conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]]], tools: Optional[List[Dict]] = None, documents: Optional[List[Dict[str, str]]] = None, chat_template: Optional[str] = None, add_generation_prompt: bool = False, continue_final_message: bool = False, tokenize: bool = True, padding: bool = False, truncation: bool = False, max_length: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_dict: bool = False, return_assistant_tokens_mask: bool = False, tokenizer_kwargs: Optional[Dict[str, Any]] = None, **kwargs) -> Union[str, List[int], List[str], List[List[int]], transformers.tokenization_utils_base.BatchEncoding] method of transformers.tokenization_utils_fast.PreTrainedTokenizerFast instance\n",
      "    Converts a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\n",
      "    ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\n",
      "    determine the format and control tokens to use when converting.\n",
      "    \n",
      "    Args:\n",
      "        conversation (Union[List[Dict[str, str]], List[List[Dict[str, str]]]]): A list of dicts\n",
      "            with \"role\" and \"content\" keys, representing the chat history so far.\n",
      "        tools (`List[Dict]`, *optional*):\n",
      "            A list of tools (callable functions) that will be accessible to the model. If the template does not\n",
      "            support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n",
      "            giving the name, description and argument types for the tool. See our\n",
      "            [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)\n",
      "            for more information.\n",
      "        documents (`List[Dict[str, str]]`, *optional*):\n",
      "            A list of dicts representing documents that will be accessible to the model if it is performing RAG\n",
      "            (retrieval-augmented generation). If the template does not support RAG, this argument will have no\n",
      "            effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys. Please\n",
      "            see the RAG section of the [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#arguments-for-RAG)\n",
      "            for examples of passing documents with chat templates.\n",
      "        chat_template (`str`, *optional*):\n",
      "            A Jinja template to use for this conversion. It is usually not necessary to pass anything to this\n",
      "            argument, as the model's template will be used by default.\n",
      "        add_generation_prompt (bool, *optional*):\n",
      "            If this is set, a prompt with the token(s) that indicate\n",
      "            the start of an assistant message will be appended to the formatted output. This is useful when you want to generate a response from the model.\n",
      "            Note that this argument will be passed to the chat template, and so it must be supported in the\n",
      "            template for this argument to have any effect.\n",
      "        continue_final_message (bool, *optional*):\n",
      "            If this is set, the chat will be formatted so that the final\n",
      "            message in the chat is open-ended, without any EOS tokens. The model will continue this message\n",
      "            rather than starting a new one. This allows you to \"prefill\" part of\n",
      "            the model's response for it. Cannot be used at the same time as `add_generation_prompt`.\n",
      "        tokenize (`bool`, defaults to `True`):\n",
      "            Whether to tokenize the output. If `False`, the output will be a string.\n",
      "        padding (`bool`, defaults to `False`):\n",
      "            Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`.\n",
      "        truncation (`bool`, defaults to `False`):\n",
      "            Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\n",
      "        max_length (`int`, *optional*):\n",
      "            Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\n",
      "            not specified, the tokenizer's `max_length` attribute will be used as a default.\n",
      "        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "            If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\n",
      "            values are:\n",
      "            - `'tf'`: Return TensorFlow `tf.Tensor` objects.\n",
      "            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "            - `'np'`: Return NumPy `np.ndarray` objects.\n",
      "            - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
      "        return_dict (`bool`, defaults to `False`):\n",
      "            Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n",
      "        tokenizer_kwargs (`Dict[str: Any]`, *optional*): Additional kwargs to pass to the tokenizer.\n",
      "        return_assistant_tokens_mask (`bool`, defaults to `False`):\n",
      "            Whether to return a mask of the assistant generated tokens. For tokens generated by the assistant,\n",
      "            the mask will contain 1. For user and system tokens, the mask will contain 0.\n",
      "            This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\n",
      "        **kwargs: Additional kwargs to pass to the template renderer. Will be accessible by the chat template.\n",
      "    \n",
      "    Returns:\n",
      "        `Union[List[int], Dict]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n",
      "        output is ready to pass to the model, either directly or via methods like `generate()`. If `return_dict` is\n",
      "        set, will return a dict of tokenizer outputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.apply_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86ee8229-fe0e-4549-9373-ef4ae51c3b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d289b58-bd66-4e38-aadc-4cac13a3cd51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
