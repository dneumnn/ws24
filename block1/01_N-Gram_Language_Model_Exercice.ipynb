{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c312b6fd-8064-44bc-b644-5c98859fcfb6",
   "metadata": {},
   "source": [
    "# Build an N-gram Language Model\n",
    "\n",
    "## Definition\n",
    "\n",
    "A language model is a probability distribution over sequences of a vocabulary.\n",
    "\n",
    "Let **V** be a vocabulary, items could be characters, tokens or words. \n",
    "\n",
    "A language model assigns a probability to each sequence of items:\n",
    "\n",
    "- A language model LM should predict the next word of a sequence of words (based on the known history).\n",
    "- We would like a language model to assign higher probabilities to sentences that are real and syntactically correct.\n",
    "\n",
    "Let $(w_1, w_2, ... ,w_n)$ be a sequence of words from vocabulary **V** and **LM** a language model over **V** with probablity **P** then:\n",
    "\n",
    " $P(w_{n+1}|w_1, ... ,w_n) = LM(w_{n+1})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d8d42d-b200-4a7e-b1b8-0d2a72896394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython Magic: https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6a58e-626d-4f0e-8872-bb93e110e104",
   "metadata": {},
   "source": [
    "## Data: Load some data\n",
    "\n",
    "Load the book Alice in Wonderland from Gutenberg: https://www.gutenberg.org/ebooks/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47b5c70-6695-441d-8c2b-d60a267c818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144694\n",
      "144586\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "url=\"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "filename = \"./data/ALICE/alice.txt\"\n",
    "\n",
    "urlretrieve(url=url, filename=filename) #Retrive a URL into a file on disk\n",
    "\n",
    "\n",
    "with open(filename, \"r\") as f:\n",
    "    data = f.read()\n",
    "    print(len(data))\n",
    "    start = data.find(\"[Illustration]\")\n",
    "    end = data.find(\"*** END OF THE PROJECT GUTENBERG EBOOK\")\n",
    "\n",
    "    data = data[start+len(\"[Illustration]\"):end]\n",
    "    print(len(data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532ef77-3180-4d89-a20a-b28e7b1618dd",
   "metadata": {},
   "source": [
    "## Corps: Build a simple corpus with NLTK\n",
    "\n",
    "A corpus is a list of sentences. Each sentence s a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab77de8-8e13-4197-8e90-7cd64ddefe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76b804e-09a2-4910-b704-30fa3066d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985 sentences found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/done/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "# nltk.sent_tokenize needs resource punkt_tab. Use the NLTK Downloader to obtain the resource:\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "corpus = [] #corpus as a list of list\n",
    "\n",
    "sentences = nltk.sent_tokenize(data) \n",
    "\n",
    "print(f\"{len(sentences)} sentences found\")\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    corpus.append(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb8b0c-2440-4cb2-8920-450814cd088e",
   "metadata": {},
   "source": [
    "## Vocabulary: Build a vocabulary\n",
    "\n",
    "A vocabulary is a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaba705a-be99-4d6c-b677-3e1398693092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book Alice in wonderland contains 985 sentences with 34636 words.\n",
      "A language model based on the book has a vocabulray with 3284 different words.\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "words = []\n",
    "for s in corpus:\n",
    "    for w in s:\n",
    "        vocabulary.add(w)\n",
    "        words.append(w)\n",
    "print(f\"The book Alice in wonderland contains {len(corpus)} sentences with {len(words)} words.\")   \n",
    "print(f\"A language model based on the book has a vocabulray with {len(vocabulary)} different words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e0cfe-fa4b-4459-a569-a9bd07460339",
   "metadata": {},
   "source": [
    "## Uni-gram\n",
    "Uni-gram is a LM where each word is independent of all other words. There is no history:\n",
    "\n",
    "$$ P(w_1, w_2, ... ,w_n) = P(w_1) * P(w_2)* ... * P(w_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ee4e0f-dd8d-4e37-b349-8818e2d7806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the word Alice is P('Alice') = 0.01149093428802402\n",
      "The sentence 'Alice lives in Wonderland' has the Probability P('Alice')*P('lives')*P('in')*P('Wonderland') = 1.1747803029063786e-12\n",
      "OCC('Alice')      = 398\n",
      "OCC('lives')      = 4\n",
      "OCC('in')         = 354\n",
      "OCC('Wonderland') = 3\n",
      "P('Alice')        = 0.01149093428802402\n",
      "P('lives')        = 0.00011548677676406051\n",
      "P('in')           = 0.010220579743619356\n",
      "P('Wonderland')   = 8.661508257304539e-05\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(words) # Counter counts the occurence of each item in a list of items.\n",
    "\n",
    "uni_gram = {w: counter[w]/len(words) for w in counter}\n",
    "\n",
    "print(f\"The probability of the word Alice is P('Alice') = {uni_gram['Alice']}\")\n",
    "print(f\"The sentence 'Alice lives in Wonderland' has the Probability P('Alice')*P('lives')*P('in')*P('Wonderland') \\\n",
    "= {uni_gram['Alice']*uni_gram['lives']*uni_gram['in']*uni_gram['Wonderland']}\")\n",
    "\n",
    "\n",
    "print(f\"OCC('Alice')      = {counter['Alice']}\")\n",
    "print(f\"OCC('lives')      = {counter['lives']}\")\n",
    "print(f\"OCC('in')         = {counter['in']}\")\n",
    "print(f\"OCC('Wonderland') = {counter['Wonderland']}\")\n",
    "\n",
    "\n",
    "print(f\"P('Alice')        = {uni_gram['Alice']}\")\n",
    "print(f\"P('lives')        = {uni_gram['lives']}\")\n",
    "print(f\"P('in')           = {uni_gram['in']}\")\n",
    "print(f\"P('Wonderland')   = {uni_gram['Wonderland']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d26b3-3817-47f0-a97d-5ad86d561863",
   "metadata": {},
   "source": [
    "## N-gram\n",
    "N-gram is a Language Model that knows all sequences up to length N:\n",
    "$$ P(w_1, w_2, ... ,w_n) = P(w_1) * P(w_2|w_1)* ... * P(w_n|w_1, ... ,w_{n-1})$$\n",
    "We can compute the probabilty for the next word given a history $(w_1, w_2, ... ,w_{n-1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31fc3059-6d84-4e4d-b4c8-4e9133fb48c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use '#' as separator\n",
    "\"#\" in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2491fe-973f-414e-99dc-4941491c1478",
   "metadata": {},
   "source": [
    "## Build a 2-gram LM based on the corpus of alice\n",
    "\n",
    "Use 2-item sequences separated by '#'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9482d899-32fb-48dd-81b5-8d959866f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 2-item sequences\n",
    "bi_gram = {}\n",
    "x = 0\n",
    "for i in range(len(words)-1):\n",
    "    seq = \"#\".join(words[i:i+2])\n",
    "    x += 1\n",
    "    if seq not in bi_gram:\n",
    "        bi_gram[seq] = 0\n",
    "    bi_gram[seq] += 1\n",
    "bi_gram = {w: bi_gram[w]/x for w in bi_gram}    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aff1a3-6a9f-4fce-90a2-63753833993e",
   "metadata": {},
   "source": [
    "### Build a 3-gram LM based on the corpus of alice\n",
    "\n",
    "Use 3-item sequences separated by '#'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86346c58-95b8-4e76-82af-30fbe346568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_gram = {}\n",
    "x = 0\n",
    "for i in range(len(words)-2):\n",
    "    seq = \"#\".join(words[i:i+3])\n",
    "    x += 1\n",
    "    if seq not in tri_gram:\n",
    "        tri_gram[seq] = 0\n",
    "    tri_gram[seq] += 1\n",
    "tri_gram = {w: tri_gram[w]/x for w in tri_gram}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6faae703-5d4f-43d2-a9c4-5985337ef15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('Alice', 'was') is 0.000490832972426736 using the 2-gram LM\n",
      "Alice#was#beginning 5.774672287347693e-05\n",
      "Alice#was#not 8.662008431021539e-05\n",
      "Alice#was#soon 2.8873361436738465e-05\n",
      "Alice#was#so 2.8873361436738465e-05\n",
      "Alice#was#more 2.8873361436738465e-05\n",
      "Alice#was#just 2.8873361436738465e-05\n",
      "Alice#was#a 2.8873361436738465e-05\n",
      "Alice#was#silent 2.8873361436738465e-05\n",
      "Alice#was#rather 2.8873361436738465e-05\n",
      "Alice#was#very 5.774672287347693e-05\n",
      "Alice#was#too 2.8873361436738465e-05\n",
      "Alice#was#thoroughly 2.8873361436738465e-05\n",
      "Alice#was#only 2.8873361436738465e-05\n",
      "P('Alice', 'was') is 0.0004908471444245538 using the 3-gram LM\n"
     ]
    }
   ],
   "source": [
    "# Get all 3-grams starting with \"Alice was\" to compute P(\"Alice\", \"was\")\n",
    "x = 0\n",
    "seqs = []\n",
    "seq = \"#\".join(['Alice','was'])\n",
    "\n",
    "print(f\"P('Alice', 'was') is {bi_gram[seq]} using the 2-gram LM\")\n",
    "\n",
    "\n",
    "for s in tri_gram:\n",
    "    if s.startswith(seq):\n",
    "        print(s, tri_gram[s])\n",
    "        seqs.append(s.split(\"#\"))\n",
    "        x += tri_gram[s]\n",
    "print(f\"P('Alice', 'was') is {x} using the 3-gram LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8edae87e-5458-40a2-8006-e24077438890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01149093428802402"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_gram[\"Alice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9626bd04-d628-4df7-93c7-db0dd66dbfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('beginning'|'Alice','was') 0.11765045568958078\n",
      "P('not'|'Alice','was') 0.17647568353437118\n",
      "P('soon'|'Alice','was') 0.05882522784479039\n",
      "P('so'|'Alice','was') 0.05882522784479039\n",
      "P('more'|'Alice','was') 0.05882522784479039\n",
      "P('just'|'Alice','was') 0.05882522784479039\n",
      "P('a'|'Alice','was') 0.05882522784479039\n",
      "P('silent'|'Alice','was') 0.05882522784479039\n",
      "P('rather'|'Alice','was') 0.05882522784479039\n",
      "P('very'|'Alice','was') 0.11765045568958078\n",
      "P('too'|'Alice','was') 0.05882522784479039\n",
      "P('thoroughly'|'Alice','was') 0.05882522784479039\n",
      "P('only'|'Alice','was') 0.05882522784479039\n",
      "1.0000288733614366\n"
     ]
    }
   ],
   "source": [
    "test = 0 # the sum of all conditional probabilities should be 1\n",
    "for _seq in seqs:\n",
    "    p123 = tri_gram[\"#\".join(_seq)]\n",
    "    p1 = uni_gram[_seq[0]]\n",
    "    p12 = bi_gram[\"#\".join(_seq[0:2])]\n",
    "    p21 = p12/p1\n",
    "    p312 = p123/(p1*p21)\n",
    "    test += p312\n",
    "    print(f\"P('{_seq[2]}'|'{_seq[0]}','{_seq[1]}')\",p312) #, f\"={p123}/({p1}*{p21})\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d2172-3bc0-43f3-b7cb-50958de9ca07",
   "metadata": {},
   "source": [
    "## Exercise: Make a next word predictor based on bi-gram and tri-gram\n",
    "#\n",
    "# 1. Build a next word predictor based on bi-gram model\n",
    "# 2. Build a next word predictor based in tri-gram model\n",
    "# Think about what happend if a probailiy is zero?\n",
    "# 3. Build a generation method that writes a little story based on bi- and tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "274c0b72-2eaa-4861-bf4d-71177a9eebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ bi-gram ###############\n",
    "#\n",
    "# p(x_1, x_2) = p(x_1) * p(x_2|x_1)\n",
    "# notation: p12 = p1 * p21\n",
    "#\n",
    "####################################\n",
    "\n",
    "def predict_next_bigram_word(start):\n",
    "    seqs = []\n",
    "    seq = start\n",
    "    for s in bi_gram:\n",
    "        if s.startswith(f\"{seq}#\"):\n",
    "            p12 = bi_gram[s]\n",
    "            p1 = uni_gram[start]\n",
    "            p21 = p12/p1\n",
    "\n",
    "            seqs.append((s.split(\"#\"), p21))\n",
    "            \n",
    "    seqs = sorted(seqs, key=lambda x:x[1], reverse=True)\n",
    "    if len(seqs) > 0:\n",
    "        s, p = seqs[0]   \n",
    "        return s[1], p\n",
    "    else:\n",
    "        return None, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2e02bb1-37b4-4964-968e-5724a00278bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 0.08000230980222318)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_bigram_word(\"was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7be23b5-f0f9-46a2-a8f3-7de287e1f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(history):\n",
    "    seqs = []\n",
    "    seq = \"#\".join(history)\n",
    "    for s in tri_gram:\n",
    "        if s.startswith(f\"{seq}#\"):\n",
    "            \n",
    "            p123 = tri_gram[s]\n",
    "            p1 = uni_gram[history[0]]\n",
    "            p12 = bi_gram[\"#\".join(history)]\n",
    "            p21 = p12/p1\n",
    "            p312 = p123/(p1*p21)\n",
    "\n",
    "            seqs.append((s.split(\"#\"), p312))\n",
    "            \n",
    "    seqs = sorted(seqs, key=lambda x:x[1], reverse=True)\n",
    "    if len(seqs) > 0:\n",
    "        s, p = seqs[0]   \n",
    "        return s[2], p\n",
    "    else:\n",
    "        return None, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ce0541f-2256-4ae4-8169-127cbdd8aa3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('not', 0.17647568353437118)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word(['Alice', 'was'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9eec348-1cb2-41d6-9d86-8fe7cfda69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def make_some_story(story:list, uni_gram, max_len=10):\n",
    "    if len(story) == 0:\n",
    "        init = list(uni_gram.keys())\n",
    "        shuffle(init)\n",
    "        story.append(init[0])\n",
    "    if len(story) == 1:\n",
    "        next, _ = predict_next_bigram_word(story[0])\n",
    "        story.append(next)\n",
    "    history = [story[-2], story[-1]]\n",
    "    #print(history)\n",
    "    next, _ = predict_next_word(history)\n",
    "    story.append(next)\n",
    "    if len(story) < max_len:\n",
    "       make_some_story(story=story, uni_gram=uni_gram, max_len=max_len)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de8264f9-e000-40ac-9f57-8717cee50a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usually', 'bleeds', ';', 'and', 'the', 'Queen', ',', 'who', 'was', 'trembling']\n"
     ]
    }
   ],
   "source": [
    "story = []\n",
    "make_some_story(story=story, uni_gram=uni_gram, max_len=10)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac255f-acda-48a9-b17b-27ddb2a3569c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
