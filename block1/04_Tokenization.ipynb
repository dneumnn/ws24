{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04deee57-9cc1-42d8-bcf5-dabdb661f865",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "see: https://en.wikipedia.org/wiki/Byte_pair_encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea4feeba-e40e-4061-9211-b2ad03c043f6",
   "metadata": {},
   "source": [
    "## tiktoken\n",
    "\n",
    "tiktoken is a fast BPE tokeniser for use with OpenAI's models:\n",
    "- 'gpt2'\n",
    "- 'r50k_base'\n",
    "- 'p50k_base'\n",
    "- 'p50k_edit'\n",
    "- ‘cl100k_base'\n",
    "- 'o200k_base’\n",
    "\n",
    "Further reading:\n",
    "- see: https://github.com/openai/tiktoken\n",
    "- see: https://pypi.org/project/tiktoken/\n",
    "- see: https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "- see: https://stackoverflow.com/questions/76106366/how-to-use-tiktoken-in-offline-mode-computer\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b726669-01f7-4131-bff7-9a42900625f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/block1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f3335f-e57a-4241-ae30-050838aecd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"This is a text about Albert Einstein\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9641d661-94bb-43ae-bbcc-0bcfd95d1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8505d82d-4fa6-4738-b92a-a371509474cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500, 382, 261, 2201, 1078, 40833, 83400]\n",
      "This is a text about Albert Einstein\n"
     ]
    }
   ],
   "source": [
    "o200k_tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "tokens = o200k_tokenizer.encode(prompt)\n",
    "print(tokens)\n",
    "prompt = o200k_tokenizer.decode(tokens)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f4a8e5a-a5d6-49cf-b8b0-72b721b42083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endofprompt|>', '<|endoftext|>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o200k_tokenizer.special_tokens_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1282472a-a944-44eb-b13e-256c31d41015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 257, 2420, 546, 9966, 24572]\n",
      "This is a text about Albert Einstein\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = gpt2_tokenizer.encode(prompt)\n",
    "print(tokens)\n",
    "prompt = gpt2_tokenizer.decode(tokens)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50aada71-fa3d-413d-be99-fac4240bc026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer.special_tokens_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8316b21a-d47c-4493-8704-c6b5f7464cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b665493-77cd-4197-82fb-9cc4b30b05f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> !\n",
      "1 --> \"\n",
      "2 --> #\n",
      "3 --> $\n",
      "4 --> %\n",
      "5 --> &\n",
      "6 --> '\n",
      "7 --> (\n",
      "8 --> )\n",
      "9 --> *\n",
      "1000 --> ale\n",
      "1001 -->  Se\n",
      "1002 -->  If\n",
      "1003 --> //\n",
      "1004 -->  Le\n",
      "1005 -->  ret\n",
      "1006 -->  ref\n",
      "1007 -->  trans\n",
      "1008 --> ner\n",
      "1009 --> ution\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "for idx in range(gpt2_tokenizer.n_vocab):\n",
    "    vocabulary[idx] = gpt2_tokenizer.decode([idx])\n",
    "\n",
    "for idx in range(10):\n",
    "    print(idx,\"-->\",vocabulary[idx])\n",
    "for idx in range(1000,1010):\n",
    "    print(idx,\"-->\",vocabulary[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23844a-fb29-479e-a801-0ae8d1c560db",
   "metadata": {},
   "source": [
    "## SentencePiece\n",
    "\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE)) and unigram language model) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n",
    "\n",
    "- see: https://github.com/google/sentencepiece\n",
    "- see: https://pypi.org/project/sentencepiece/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa9be1ae-db08-4344-8bd8-bfd02372e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d753396-d2b7-4f41-b03c-8c839ec9c0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/ALICE/alice.txt\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/ALICE/alice.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 3383 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=143222\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9539% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999539\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 2690 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=77144\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 7443 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 2690\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 5269\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 5269 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3138 obj=10.645 num_tokens=11303 num_tokens/piece=3.60198\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2749 obj=8.79056 num_tokens=11360 num_tokens/piece=4.13241\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=8.81782 num_tokens=11930 num_tokens/piece=5.42273\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2199 obj=8.77721 num_tokens=11931 num_tokens/piece=5.42565\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sentencepiece as spm\n",
    "\n",
    "model = io.BytesIO()\n",
    "spm.SentencePieceTrainer.train(input='data/ALICE/alice.txt', model_writer=model, vocab_size=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e84c1f2-a97e-4376-be9c-acc30758c206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 60, 17]\n",
      "this is Alice\n"
     ]
    }
   ],
   "source": [
    "# Serialize the model as file.\n",
    "with open('data/ALICE/alice.model', 'wb') as f:\n",
    "   f.write(model.getvalue())\n",
    "\n",
    "# Directly load the model from serialized model.\n",
    "tokenizer = spm.SentencePieceProcessor(model_proto=model.getvalue())\n",
    "tokens = tokenizer.encode('this is Alice')\n",
    "print(tokens)\n",
    "prompt = tokenizer.decode(tokens)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982ea6e-524b-4c66-ae4f-ce24c3ef8388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
