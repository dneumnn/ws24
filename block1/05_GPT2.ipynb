{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22535736-4724-4bab-acab-032ca2c58488",
   "metadata": {},
   "source": [
    "# Understanding GPT2\n",
    "\n",
    "see: https://huggingface.co/openai-community/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32db8181-8247-49c7-a0ff-996c0f66a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b45a51-b912-4829-8fd5-c6ce4045f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03de15-7c3e-4e2a-9c90-c385cbe3c536",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfac9d2b-c6be-4cf4-b28d-06459820aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, clean_up_tokenization_spaces=True)\n",
    "\n",
    "########## Let have a look at the vocabulary of GPT-2 ##########\n",
    "vocab = tokenizer.get_vocab() #key=subword, value=token_id\n",
    "print(type(vocab))\n",
    "print(len(vocab))\n",
    "vocab_as_list = [subword for subword, _ in vocab.items()]\n",
    "tokens_as_list = [token_id for _, token_id in vocab.items()]\n",
    "token_dict = {token_id:subword for subword, token_id in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa28eab-9639-4e0d-a2bf-4d54cb46bd4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799cd4f8-dd64-4a24-bb90-4ae2c2f0e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The theory of relativity is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7278255-f911-48b3-841d-03750a520337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[464, 4583, 286, 44449, 318]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(prompt)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3293842c-c983-4a9a-8fca-6976f9649247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 \t-> 'The'\n",
      "4583 \t-> ' theory'\n",
      "286 \t-> ' of'\n",
      "44449 \t-> ' relativity'\n",
      "318 \t-> ' is'\n"
     ]
    }
   ],
   "source": [
    "for token_id in input_ids:\n",
    "    subword = tokenizer.decode(token_id)\n",
    "    print(token_id, \"\\t->\", f\"'{subword}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d168d4-45ff-4c46-8ff8-685b2d747f2b",
   "metadata": {},
   "source": [
    "## Plain GPT2 \n",
    "![without_language_model_head](./media/without_language_model_head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a7ea8ab-aafa-4e9a-b970-e65c2e62f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c402304f-b7bc-45db-b0c2-ddefc4979800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0dc591-2e20-4c8e-9d12-e5722e5e22e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  The theory of relativity is\n",
      "tokens:  [464, 4583, 286, 44449, 318]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0470, -0.0333, -0.1626,  ..., -0.1337, -0.0571, -0.1059],\n",
       "         [-0.4281, -0.1663, -0.9736,  ...,  0.0973,  0.1560, -0.3451],\n",
       "         [ 0.4888, -0.2045, -0.7373,  ..., -0.1467,  0.2325,  0.1179],\n",
       "         [-0.3101, -0.0585, -0.5952,  ...,  0.4587,  0.2593, -0.4875],\n",
       "         [-0.3415, -0.0268, -1.4781,  ...,  0.4598,  0.1627, -0.1709]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"prompt: \", prompt)\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"tokens: \", tokenizer.encode(prompt))\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, output_attentions=False)\n",
    "last_hidden_state = outputs.last_hidden_state # returns the logits of last hidden state\n",
    "last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc39465-d2bc-4442-956d-7ab2457d879e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape # batch, sequence, embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d57d5d-d689-434a-9c98-ea127f87535e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f355c3a8-94cd-46bf-b292-58f5eaea038e",
   "metadata": {},
   "source": [
    "## Understanding Text Generation\n",
    "For \"next word prediction\" put a language model head on top of GPT2: Use GPT2LMHeadModel, since this adds the language modeling head on top of GPT2Model.\n",
    "![with_language_model_head](./media/with_language_model_head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "399f0fdf-4ae1-480b-8ff2-68cb2bf3198b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "888e998d-1021-488f-ab9b-92be546a7661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50257])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The theory of relativity is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids)\n",
    "logits = outputs.logits\n",
    "print(logits.shape)\n",
    "next_token_logits = logits[0, -1,:] #log probs\n",
    "next_token_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5a4e8-b2be-49f2-9334-a5aba4175229",
   "metadata": {},
   "source": [
    "### Use Softmax to convert logits into probabilities\n",
    "\n",
    "Softmax function applied to an n-dimensional input vector rescales them so that the elements of the n-dimensional output Tensor lie in the range $[0,1]$ and sum to 1.\n",
    "\n",
    "Softmax is defined pointwise as:\n",
    "\n",
    "$$Softmax(x_i) = \\frac{exp(x_i)}{\\sum_{j} exp(x_j)} $$\n",
    "\n",
    "see: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b811066a-41a3-4d5a-86d9-f10a48cf0321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "# convert the logits into probabilities\n",
    "next_token_probs = torch.softmax(next_token_logits, -1)\n",
    "print(next_token_probs.shape) # for each token in the vocabulary we get a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59e9a0e-8a1a-49af-9ebc-9c08fdc0fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get the top 10 and put all together:\n",
      "prob \t  token\n",
      "---------------\n",
      "0.381 \t  that\n",
      "0.081 \t  based\n",
      "0.04 \t  the\n",
      "0.038 \t  a\n",
      "0.028 \t  not\n",
      "0.019 \t  simple\n",
      "0.013 \t  to\n",
      "0.01 \t ,\n",
      "0.01 \t  very\n",
      "0.009 \t  one\n"
     ]
    }
   ],
   "source": [
    "print(\"get the top 10 and put all together:\")\n",
    "print(\"prob\",\"\\t\",\" token\")\n",
    "print(\"---------------\")\n",
    "topk_next_tokens= torch.topk(next_token_probs, 10) \n",
    "topk_next_token_list = [(tokenizer.decode(idx), prob) for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values)] \n",
    "for token, prob in topk_next_token_list:\n",
    "    print(round(prob.item(),3),\"\\t\",token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93911f3b-1306-4cea-92fd-e02000c2c68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 326 with probability 0.38146549463272095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' that'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use argmax to get the index of the vector with the highest probability\n",
    "index = torch.argmax(next_token_probs)\n",
    "next_token = index.item()\n",
    "print(\"index\", next_token, \"with probability\", next_token_probs[next_token].item()) #index == token_id\n",
    "tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8f7865c-af9f-4ac1-9c2a-3151eee12355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theory of relativity is that the speed of light is proportional to the distance between two points.\n"
     ]
    }
   ],
   "source": [
    "############# greedy search loop ###########\n",
    "prompt = \"The theory of relativity\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "max_new_tokens = 15\n",
    "for i in range(max_new_tokens):\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    next_token_logits = outputs.logits[0, -1,:]\n",
    "    next_token_probs = torch.softmax(next_token_logits, -1)\n",
    "    index = torch.argmax(next_token_probs)\n",
    "    prompt = prompt + tokenizer.decode(index.item())\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adfe5ac-43bf-408b-8444-8c588122abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Exercise: Implement a beam search \n",
    "#\n",
    "# see: https://en.wikipedia.org/wiki/Beam_search\n",
    "#\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e69be-e1d8-4173-ad01-3baeb5489498",
   "metadata": {},
   "source": [
    "## Text Generation Strategies\n",
    "\n",
    "- beam search\n",
    "- top k\n",
    "- top p\n",
    "- temperature\n",
    "\n",
    "see: https://lena-voita.github.io/nlp_course/language_modeling.html#generation_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df7674c6-d431-4e45-91dd-a885d3f9dd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The theory of relativity is based on the idea that the speed of light is proportional to the distance between two points on a sphere. The speed of light is proportional to the distance'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# Generate Text ###########\n",
    "# the generate method is dooing the looping\n",
    "prompt = \"The theory of relativity is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "outputs = model.generate(input_ids, attention_mask=attention_mask,\n",
    "                         pad_token_id = tokenizer.eos_token_id,\n",
    "                         max_new_tokens=30, num_beams=2)\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1df0925-664d-4754-8e5b-bb53c4174b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The theory of relativity is that when we measure the curvature in space as given by Einstein, and take account of its angular momentum with respect to the observer's point of view (i.e., where all two objects are stationary), one can say that an object has a velocity equal to 2E-3^7*2/4 * \\\\pi_{j=\\\\frac{t}{s}\\\\rightarrow S&S}(T). We have shown before how this law was already known for other celestial bodies such\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use temperature, ...\n",
    "prompt = \"The theory of relativity is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "outputs = model.generate(input_ids, attention_mask=attention_mask,\n",
    "                         pad_token_id = tokenizer.eos_token_id,\n",
    "                         max_new_tokens=100, num_beams=4, \n",
    "                         temperature=2.0, do_sample=True,\n",
    "                         repetition_penalty=10.0\n",
    "                        )\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1bfa6-ee40-4313-bcf0-d4b9d0c62b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
