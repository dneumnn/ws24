{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03bd42e8-1846-48a3-84cd-97e35075da5f",
   "metadata": {},
   "source": [
    "# WS24 - Intelligente Informationssysteme\n",
    "\n",
    "## Block 3: Retrieval Augmented Generation\n",
    "\n",
    "**Part 8: Advanced Retrieval - Retrieval**\n",
    "\n",
    "![Retrieval](./media/LangChain_Retrieval.png \"Retrieval\")\n",
    "\n",
    "1. Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055de8ed-7a7b-494e-a5d5-7585c4d1ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIRST: Initialize the VectorDB and LLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(persist_directory=\"vector_store\", collection_name=\"lils_blogs\", embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})\n",
    "\n",
    "## LLM with function calling ability\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ba0a671-0736-46ad-9181-8aa4b192b2fa",
   "metadata": {},
   "source": [
    "## Re-Ranking\n",
    "\n",
    "![Re-Ranking](./media/LangChain_Re-Ranking.png \"Re-Ranking\")\n",
    "\n",
    "\n",
    "see: \n",
    "- https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_10_and_11.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821e773d-73fd-4317-8a6c-bbc4dfa735e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to task decomposition?\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "initial_ranked = [{\"text\": doc.page_content, \"cross-encoder_score\":0, \"id\": i} for i, doc in enumerate(retrieved_docs)]\n",
    "\n",
    "len(initial_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289bd92-f4f1-40ca-90d2-aeb291e50e28",
   "metadata": {},
   "source": [
    "### Re-Ranking with Cross-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96c5cd-b5bd-4e8d-b2d4-d5b8c347cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d201da-8a27-4874-ada7-7be73dda4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# from https://medium.com/@rossashman/the-art-of-rag-part-3-reranking-with-cross-encoders-688a16b64669\n",
    "def reranker(query, hits):\n",
    "        \n",
    "    # To refine the results, we use a CrossEncoder. A CrossEncoder gets both inputs (input_question, retrieved_question)\n",
    "    # and outputs a score 0...1 indicating the similarity.\n",
    "    cross_encoder_model = CrossEncoder(\"cross-encoder/stsb-roberta-base\")\n",
    "\n",
    "    # Now, do the re-ranking with the cross-encoder\n",
    "    sentence_pairs = [[query, hit[\"text\"]] for hit in hits]\n",
    "    similarity_scores = cross_encoder_model.predict(sentence_pairs)\n",
    "    \n",
    "    for idx in range(len(hits)):\n",
    "        hits[idx][\"cross-encoder_score\"] = similarity_scores[idx]\n",
    "\n",
    "    # Sort list by CrossEncoder scores\n",
    "    #hits = sorted(hits, key=lambda x: x[\"cross-encoder_score\"], reverse=True)\n",
    "    #print(\"Top 5 hits with CrossEncoder:\")\n",
    "    #for hit in hits:\n",
    "    #    print(\"\\t{:.3f}\\t{}\".format(hit[\"cross-encoder_score\"], hit[\"id\"]))\n",
    "\n",
    "    #print(\"\\n\\n========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b5cef6-28a8-421e-a603-b659bbdee4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker(question, initial_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66060c55-2ccf-4401-98d4-5abc87d66e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_ranked = sorted(initial_ranked, key=lambda x:x[\"cross-encoder_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bea5a4-6121-4872-9af6-8bc4994aed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-encoder_score     id\n",
      "\t0.607\t\t18\n",
      "\t0.543\t\t19\n",
      "\t0.507\t\t6\n",
      "\t0.499\t\t7\n",
      "\t0.498\t\t2\n",
      "\t0.471\t\t0\n",
      "\t0.470\t\t9\n",
      "\t0.454\t\t16\n",
      "\t0.441\t\t17\n",
      "\t0.438\t\t11\n",
      "\t0.437\t\t10\n",
      "\t0.424\t\t12\n",
      "\t0.415\t\t13\n",
      "\t0.400\t\t1\n",
      "\t0.388\t\t14\n",
      "\t0.387\t\t5\n",
      "\t0.387\t\t4\n",
      "\t0.262\t\t15\n",
      "\t0.213\t\t3\n",
      "\t0.197\t\t8\n"
     ]
    }
   ],
   "source": [
    "print(\"cross-encoder_score\", \"    id\")\n",
    "for hit in re_ranked:\n",
    "    print(\"\\t{:.3f}\\t\\t{}\".format(hit[\"cross-encoder_score\"], hit[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08714379-9b0e-4527-996a-eb6e4e7da324",
   "metadata": {},
   "source": [
    "### Re-Ranking with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108803d-53e0-4845-8e94-d72aa0dc0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-retrievers-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb30a9f-0dab-4e63-93b4-6e744829f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://docs.llamaindex.ai/en/stable/examples/retrievers/bm25_retriever/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4c490f-77eb-453b-8b09-6a7decfd5ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "#from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = [Document(text=doc.page_content, metadata={\"id\":i}) for i, doc in enumerate(retrieved_docs)]\n",
    "\n",
    "# parse nodes\n",
    "#parser = SentenceSplitter()\n",
    "#nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "import Stemmer\n",
    "\n",
    "# We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=documents,\n",
    "    similarity_top_k=len(documents),\n",
    "    # Optional: We can pass in the stemmer and set the language for stopwords\n",
    "    # This is important for removing stopwords and stemming the query + text\n",
    "    # The default is english for both\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dcc21e0-7a25-4c08-9d55-9926bee5bbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.retrievers.bm25.base.BM25Retriever at 0x336cfa0d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c42860b5-2daf-41fe-bdb3-99ed780cbbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0}\n",
      "{'id': 2}\n",
      "{'id': 8}\n",
      "{'id': 10}\n",
      "{'id': 16}\n",
      "{'id': 18}\n",
      "{'id': 5}\n",
      "{'id': 14}\n",
      "{'id': 4}\n",
      "{'id': 1}\n",
      "{'id': 11}\n",
      "{'id': 7}\n",
      "{'id': 12}\n",
      "{'id': 6}\n",
      "{'id': 15}\n",
      "{'id': 17}\n",
      "{'id': 19}\n",
      "{'id': 9}\n",
      "{'id': 13}\n",
      "{'id': 3}\n"
     ]
    }
   ],
   "source": [
    "retrieved_nodes = bm25_retriever.retrieve(question)\n",
    "for node in retrieved_nodes:\n",
    "    print(node.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86df59-2521-4e6c-9a47-9d1070d9bf16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
