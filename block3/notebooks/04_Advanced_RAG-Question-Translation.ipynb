{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bd42e8-1846-48a3-84cd-97e35075da5f",
   "metadata": {},
   "source": [
    "# WS24 - Intelligente Informationssysteme\n",
    "\n",
    "## Block 3: Retrieval Augmented Generation\n",
    "\n",
    "**Part 4: Advanced Retrieval - Question Translation**\n",
    "\n",
    "1. Multi Query\n",
    "2. RAG-Fusion\n",
    "3. Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055de8ed-7a7b-494e-a5d5-7585c4d1ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIRST: Initialize the VectorDB and LLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(persist_directory=\"vector_store\", collection_name=\"lils_blogs\", embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\") #, search_kwargs={\"k\": 2})\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ba0a671-0736-46ad-9181-8aa4b192b2fa",
   "metadata": {},
   "source": [
    "## 1. Question Translation: Multi Query\n",
    "\n",
    "Multi Query: Different perspectives on the same input question\n",
    "\n",
    "![Multi Query](./media/LangChain_Multi_Query.png \"Multi Query\")\n",
    "\n",
    "\n",
    "see: \n",
    "- https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb\n",
    "- https://python.langchain.com/docs/how_to/MultiQueryRetriever/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c7d5e-2bd1-427d-9898-9bb96ef74606",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newline. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2dc56-f425-4487-83ac-8ec1e21875bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LineListOutputParser split the LLM result into a list of queries\n",
    "\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "output_parser = LineListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3ccf7-fe2f-446f-9bd0-f667e7032603",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_template \n",
    "    | llm\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3031d6-a7a7-47e2-924b-7e2d3792223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Task Decomposition?\"\n",
    "\n",
    "multi_queries = generate_queries.invoke(question)\n",
    "multi_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86038b2d-6006-4277-b0ad-30aebb5ae957",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Retrieve doc for each query and aggregate \n",
    "import langchain\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "multi_docs = []\n",
    "for query in multi_queries:\n",
    "    multi_docs.append(retriever.invoke(query))\n",
    "print(len(multi_docs))\n",
    "\n",
    "new_docs = get_unique_union(multi_docs)\n",
    "print(len(new_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feccede4-5118-41cf-892c-6d48e23c4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Final RAG Generation ##########\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in new_docs])\n",
    "\n",
    "runnable = RunnablePassthrough(lambda x: str(x))\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": itemgetter(\"context\"), \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for answer in  final_rag_chain.stream({\"context\":context, \"question\":question}):\n",
    "    print(answer, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7eb8d-2757-4946-9c90-aab1cc183d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Do the same with MultiQueryRetriever #######\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "\n",
    "# Results\n",
    "\n",
    "new_docs = multi_query_retriever.invoke(question)\n",
    "len(new_docs)\n",
    "\n",
    "# Use the new docs and concatenate them to one context for final RAG Generation or use RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35f04b-1950-4885-b6cc-880e119480fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaa90328-b3dc-4ef7-b0b5-3e7222bf3eb3",
   "metadata": {},
   "source": [
    "## 2. RAG Fusion\n",
    "\n",
    "RAG-Fusion bridges the gap between what users explicitly ask and what they intend to ask.\n",
    "\n",
    "![RAG Fusion](./media/LangChain_RAG_Fusion.png \"RAG Fusion\")\n",
    "\n",
    "\n",
    "see: \n",
    "- https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb\n",
    "- https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119d5a3-1ada-4707-92d6-68805fd06317",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Reciprocal RAG Fusion\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "print(len(new_docs))\n",
    "rrf_docs = reciprocal_rank_fusion([new_docs]) # expects a list of lists\n",
    "print(len(rrf_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add18a8-6391-4159-b7e9-15863b690d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5 # Use the top 5 reranked documents\n",
    "new_docs = [doc for (doc, rank) in rrf_docs[0:n]]\n",
    "# Use the new docs and concatenate them to one context for final RAG Generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d2a66-74fc-4e52-9913-259f0cfe1ada",
   "metadata": {},
   "source": [
    "## 3. Decomposition\n",
    "\n",
    "Decompose the question into subqueries and answer recursively or individually:\n",
    "\n",
    "![Decomposition: Answer recursively](./media/LangChain_Decomposition1.png \"Decomposition\")\n",
    "![Decomposition: Answer individually](./media/LangChain_Decomposition2.png \"Decomposition\")\n",
    "\n",
    "see: \n",
    "- https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed204f3b-b46f-459f-b95f-8683a63d95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First decompose the question using the llm.\n",
    "    \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition Prompt\n",
    "#template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "#The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "#Generate multiple search queries related to: {question}.\\n\n",
    "#Output (3 queries):\"\"\"\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} Retrun only the list of search queries.\\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a4a7b-6e02-429e-aed8-588ccb4441b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85082b-9870-434f-91e9-f48870b7e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553fa5b6-608b-460d-9eed-faa228de8459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15bb1222-3fd9-4f67-9452-4cfccc966817",
   "metadata": {},
   "source": [
    "#### Answer Recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cc0a4-bf52-4268-8e62-1a5bb483b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final RAG Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7acece-4b6b-47fa-89a4-3edff578bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0, max_length=400)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    print(q_a_pair)\n",
    "    print(\"=\"*80)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc9719-379f-4bb7-859c-45af9727cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe26cba-aa2b-4109-9219-c7908936d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Final Answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c0866-4b66-427f-ba52-530b385e0a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6146687-1d43-4d73-b043-bec0c4c6a1fa",
   "metadata": {},
   "source": [
    "#### Answer Individually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b16bba-f6a1-4a17-ab87-8768cdcaf031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer each sub-question individually \n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# RAG prompt\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "# llm\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0, max_length=400)\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        #retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_template, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02fb75-d54a-4c1f-a97d-02af4aadb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, answer in zip(questions, answers):\n",
    "    print(f\"Question\\n {question}\\n\")\n",
    "    print(f\"Answer\\n {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fea07-e031-4435-895b-1bef5770b3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
