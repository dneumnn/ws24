{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bd42e8-1846-48a3-84cd-97e35075da5f",
   "metadata": {},
   "source": [
    "# WS24 - Intelligente Informationssysteme\n",
    "\n",
    "## Block 3: Retrieval Augmented Generation\n",
    "\n",
    "**Part 4: Advanced Retrieval - Question Translation**\n",
    "\n",
    "1. Multi Query\n",
    "2. RAG-Fusion\n",
    "3. Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055de8ed-7a7b-494e-a5d5-7585c4d1ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIRST: Initialize the VectorDB and LLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(persist_directory=\"vector_store\", collection_name=\"lils_blogs\", embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\") #, search_kwargs={\"k\": 2})\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ba0a671-0736-46ad-9181-8aa4b192b2fa",
   "metadata": {},
   "source": [
    "## 1. Question Translation: Multi Query\n",
    "\n",
    "Multi Query: Different perspectives on the same input question\n",
    "\n",
    "![Multi Query](./media/LangChain_Multi_Query.png \"Multi Query\")\n",
    "\n",
    "\n",
    "see: \n",
    "- https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb\n",
    "- https://python.langchain.com/docs/how_to/MultiQueryRetriever/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516c7d5e-2bd1-427d-9898-9bb96ef74606",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newline. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda193b4-4e90-4430-9a43-4ccba0575a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='You are an AI language model assistant. Your task is to generate five \\ndifferent versions of the given user question to retrieve relevant documents from a vector \\ndatabase. By generating multiple perspectives on the user question, your goal is to help\\nthe user overcome some of the limitations of the distance-based similarity search. \\nProvide these alternative questions separated by newline. Original question: What is Task Decomposition?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Task Decomposition?\"\n",
    "prompt_template.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc2dc56-f425-4487-83ac-8ec1e21875bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LineListOutputParser split the LLM result into a list of queries\n",
    "\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "output_parser = LineListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae296a6-376b-48b1-ac74-dcb04ab25abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are five different versions of the original question:\\n\\n1. Can you explain the concept of breaking down complex tasks into smaller, manageable components?\\n\\n2. How does task decomposition work in project management and what benefits does it provide?\\n\\n3. What is the purpose of decomposing a large task into smaller sub-tasks to improve efficiency and productivity?\\n\\n4. Is there a specific methodology or framework for task decomposition that you can recommend?\\n\\n5. Can you provide an example of how task decomposition has been applied in real-world scenarios, such as software development or operations management?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = prompt_template | llm\n",
    "\n",
    "response = llm_chain.invoke(question)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea3ccf7-fe2f-446f-9bd0-f667e7032603",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_template \n",
    "    | llm\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f3031d6-a7a7-47e2-924b-7e2d3792223b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are five different versions of the original question:',\n",
       " '1. What is the process of task decomposition, and how can it be applied to break down complex tasks into manageable components?',\n",
       " '2. Can you provide a detailed explanation of task decomposition, including its benefits and common techniques used in this approach?',\n",
       " '3. How does task decomposition work, and what are some key considerations when decomposing a task into smaller sub-tasks?',\n",
       " '4. What is the purpose of task decomposition, and how can it be used to improve project management, team collaboration, and individual productivity?',\n",
       " '5. Break down the task decomposition process step by step, including identifying goals, analyzing tasks, and creating a plan for implementation.',\n",
       " 'These alternative questions aim to capture different aspects of the original question, such as the process, benefits, considerations, purpose, and application of task decomposition. By generating multiple perspectives, we can help users like you find relevant information more effectively.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Explain Task Decomposition step by step?\"\n",
    "\n",
    "multi_queries = generate_queries.invoke(question)\n",
    "multi_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86038b2d-6006-4277-b0ad-30aebb5ae957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 questions\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "######### Retrieve doc for each query and aggregate \n",
    "import langchain\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "multi_docs = []\n",
    "for query in multi_queries:\n",
    "    multi_docs.append(retriever.invoke(query))\n",
    "print(len(multi_docs), \"questions\")\n",
    "\n",
    "new_docs = get_unique_union(multi_docs)\n",
    "print(len(new_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feccede4-5118-41cf-892c-6d48e23c4862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique used to break down complex tasks into smaller, more manageable subtasks. Here's a step-by-step explanation of task decomposition:\n",
      "\n",
      "1. **Identify the complex task**: Start by identifying the complex task that needs to be broken down.\n",
      "2. **Determine the subgoals**: Determine what the subgoals are for each subtask. Subgoals are specific, measurable outcomes that need to be achieved in order to complete the larger task.\n",
      "3. **Break down the subtasks**: Break down each subgoal into smaller subtasks. These subtasks should still be related to the original complex task and should help achieve the overall goal.\n",
      "4. **Identify dependencies**: Identify any dependencies between subtasks. This means determining which subtask needs to be completed before another can begin.\n",
      "5. **Create a tree structure**: Create a tree structure that represents the relationships between subtasks. Each node in the tree represents a subtask, and the edges represent the dependencies between them.\n",
      "6. **Evaluate each state**: Evaluate each state in the tree structure using a classifier (via a prompt) or majority vote. This helps to determine which path is most likely to lead to success.\n",
      "\n",
      "There are three ways to perform task decomposition:\n",
      "\n",
      "1. **LLM with simple prompting**: Use a large language model (LLM) with simple prompting to generate subtasks.\n",
      "2. **Zero-shot CoT**: Use zero-shot CoT, which involves generating reasoning chains and then prompting with \"Therefore, the answer is...\" to produce answers.\n",
      "3. **Self-consistency sampling**: Use self-consistency sampling, which involves sampling a number of diverse answers and then taking the majority vote.\n",
      "\n",
      "Task decomposition can be beneficial for several reasons:\n",
      "\n",
      "* It helps to reduce complexity by breaking down large tasks into smaller, more manageable subtasks.\n",
      "* It allows for more efficient use of resources by identifying dependencies between subtasks.\n",
      "* It enables better evaluation of each state in the tree structure using a classifier or majority vote.\n",
      "* It improves reasoning accuracy by sampling diverse answers and taking the majority vote.\n",
      "\n",
      "Overall, task decomposition is an important technique for breaking down complex tasks into smaller, more manageable subtasks. By following these steps, you can create a tree structure that represents the relationships between subtasks and evaluate each state using a classifier or majority vote."
     ]
    }
   ],
   "source": [
    "########## Final RAG Generation ##########\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in new_docs])\n",
    "\n",
    "runnable = RunnablePassthrough(lambda x: str(x))\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": itemgetter(\"context\"), \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for answer in  final_rag_chain.stream({\"context\":context, \"question\":question}):\n",
    "    print(answer, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9c7eb8d-2757-4946-9c90-aab1cc183d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Do the same with MultiQueryRetriever #######\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "\n",
    "# Results\n",
    "\n",
    "new_docs = multi_query_retriever.invoke(question)\n",
    "len(new_docs)\n",
    "\n",
    "# Use the new docs and concatenate them to one context for final RAG Generation or use RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35f04b-1950-4885-b6cc-880e119480fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaa90328-b3dc-4ef7-b0b5-3e7222bf3eb3",
   "metadata": {},
   "source": [
    "## 2. RAG Fusion\n",
    "\n",
    "RAG-Fusion bridges the gap between what users explicitly ask and what they intend to ask.\n",
    "\n",
    "![RAG Fusion](./media/LangChain_RAG_Fusion.png \"RAG Fusion\")\n",
    "\n",
    "\n",
    "see: \n",
    "- https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb\n",
    "- https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b119d5a3-1ada-4707-92d6-68805fd06317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "##### Reciprocal RAG Fusion\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "print(len(new_docs))\n",
    "rrf_docs = reciprocal_rank_fusion([new_docs]) # expects a list of lists\n",
    "print(len(rrf_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1add18a8-6391-4159-b7e9-15863b690d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5 # Use the top 5 reranked documents\n",
    "new_docs = [doc for (doc, rank) in rrf_docs[0:n]]\n",
    "# Use the new docs and concatenate them to one context for final RAG Generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d2a66-74fc-4e52-9913-259f0cfe1ada",
   "metadata": {},
   "source": [
    "## 3. Decomposition\n",
    "\n",
    "Decompose the question into subqueries and answer recursively or individually:\n",
    "\n",
    "![Decomposition: Answer recursively](./media/LangChain_Decomposition1.png \"Decomposition\")\n",
    "![Decomposition: Answer individually](./media/LangChain_Decomposition2.png \"Decomposition\")\n",
    "\n",
    "see: \n",
    "- https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed204f3b-b46f-459f-b95f-8683a63d95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First decompose the question using the llm.\n",
    "    \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition Prompt\n",
    "#template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "#The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "#Generate multiple search queries related to: {question}.\\n\n",
    "#Output (3 queries):\"\"\"\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} Retrun only the list of search queries.\\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb2a4a7b-6e02-429e-aed8-588ccb4441b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae85082b-9870-434f-91e9-f48870b7e3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the key components of a large language model (LLM) used in autonomous decision-making?',\n",
       " '2. How do LLMs contribute to the development of autonomous agents with natural language understanding and processing capabilities?',\n",
       " '3. What role do reinforcement learning algorithms play in integrating LLMs with autonomous agent systems for optimal performance?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553fa5b6-608b-460d-9eed-faa228de8459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15bb1222-3fd9-4f67-9452-4cfccc966817",
   "metadata": {},
   "source": [
    "#### Answer Recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c0cc0a4-bf52-4268-8e62-1a5bb483b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final RAG Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f7acece-4b6b-47fa-89a4-3edff578bded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 1. What are the key components of a large language model (LLM) used in autonomous decision-making?\n",
      "Answer: Based on the provided context, the key components of a large language model (LLM) used in autonomous decision-making include:\n",
      "\n",
      "1. Planning:\n",
      "\t* Subgoal and decomposition: Breaking down large tasks into smaller, manageable subgoals.\n",
      "\t* Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes, and refine them for future steps.\n",
      "\n",
      "These components work together to enable the LLM-powered autonomous agent system to make informed decisions and improve its performance over time.\n",
      "================================================================================\n",
      "Question: 2. How do LLMs contribute to the development of autonomous agents with natural language understanding and processing capabilities?\n",
      "Answer: Based on the provided context, Large Language Models (LLMs) contribute to the development of autonomous agents with natural language understanding and processing capabilities in several ways:\n",
      "\n",
      "1. **Planning**: LLMs can break down large tasks into smaller, manageable subgoals through subgoal and decomposition, enabling efficient handling of complex tasks. This is achieved through techniques such as Chain of Thought (CoT), which transforms big tasks into multiple simpler steps.\n",
      "2. **Reflection and refinement**: LLMs can do self-criticism and self-reflection over past actions, learn from mistakes, and refine them for future steps. This improves the quality of final results and enables the agent to adapt and improve its performance over time.\n",
      "3. **Natural Language Interface**: LLMs serve as the agent's brain, complemented by several key components, including planning and reflection. The natural language interface allows the agent to interact with external components such as memory and tools, enabling it to process and understand human input.\n",
      "\n",
      "Overall, LLMs provide a powerful general problem solver that can be used to develop autonomous agents with natural language understanding and processing capabilities. By leveraging their strengths in planning, reflection, and natural language processing, LLM-powered autonomous agents can make informed decisions, learn from mistakes, and improve their performance over time.\n",
      "================================================================================\n",
      "Question: 3. What role do reinforcement learning algorithms play in integrating LLMs with autonomous agent systems for optimal performance?\n",
      "Answer: Based on the provided context, reinforcement learning algorithms play a crucial role in integrating Large Language Models (LLMs) with autonomous agent systems for optimal performance.\n",
      "\n",
      "Reinforcement learning is used to fine-tune the LLM-powered autonomous agent system, particularly when the sampling steps are non-differentiable. The algorithm maximizes the reward $\\mathbb{E}_{\\mathbf{x} \\sim p_{red}(.)} [r(\\mathbf{x}, \\mathbf{y})]$ with a KL divergence term between the current $p_{red}$ and the initial model behavior, where $\\mathbf{y}$ is a sample from the target model.\n",
      "\n",
      "In the context of LLM-powered autonomous agent systems, reinforcement learning algorithms are used to:\n",
      "\n",
      "1. Fine-tune the model: Reinforcement learning is used to fine-tune the LLM-powered autonomous agent system, particularly when the sampling steps are non-differentiable.\n",
      "2. Maximize the reward: The algorithm maximizes the reward $\\mathbb{E}_{\\mathbf{x} \\sim p_{red}(.)} [r(\\mathbf{x}, \\mathbf{y})]$ to improve the performance of the autonomous agent system.\n",
      "3. Improve the quality of final results: By learning from mistakes and refining them for future steps, reinforcement learning algorithms help improve the quality of final results.\n",
      "\n",
      "Overall, reinforcement learning algorithms play a crucial role in integrating LLMs with autonomous agent systems for optimal performance by fine-tuning the model, maximizing the reward, and improving the quality of final results.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0, max_length=400)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    print(q_a_pair)\n",
    "    print(\"=\"*80)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfe26cba-aa2b-4109-9219-c7908936d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, reinforcement learning algorithms play a crucial role in integrating Large Language Models (LLMs) with autonomous agent systems for optimal performance.\n",
      "\n",
      "Reinforcement learning is used to fine-tune the LLM-powered autonomous agent system, particularly when the sampling steps are non-differentiable. The algorithm maximizes the reward $\\mathbb{E}_{\\mathbf{x} \\sim p_{red}(.)} [r(\\mathbf{x}, \\mathbf{y})]$ with a KL divergence term between the current $p_{red}$ and the initial model behavior, where $\\mathbf{y}$ is a sample from the target model.\n",
      "\n",
      "In the context of LLM-powered autonomous agent systems, reinforcement learning algorithms are used to:\n",
      "\n",
      "1. Fine-tune the model: Reinforcement learning is used to fine-tune the LLM-powered autonomous agent system, particularly when the sampling steps are non-differentiable.\n",
      "2. Maximize the reward: The algorithm maximizes the reward $\\mathbb{E}_{\\mathbf{x} \\sim p_{red}(.)} [r(\\mathbf{x}, \\mathbf{y})]$ to improve the performance of the autonomous agent system.\n",
      "3. Improve the quality of final results: By learning from mistakes and refining them for future steps, reinforcement learning algorithms help improve the quality of final results.\n",
      "\n",
      "Overall, reinforcement learning algorithms play a crucial role in integrating LLMs with autonomous agent systems for optimal performance by fine-tuning the model, maximizing the reward, and improving the quality of final results.\n"
     ]
    }
   ],
   "source": [
    "##### Final Answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c0866-4b66-427f-ba52-530b385e0a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6146687-1d43-4d73-b043-bec0c4c6a1fa",
   "metadata": {},
   "source": [
    "#### Answer Individually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50b16bba-f6a1-4a17-ab87-8768cdcaf031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer each sub-question individually \n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# RAG prompt\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "# llm\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0, max_length=400)\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        #retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_template, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d02fb75-d54a-4c1f-a97d-02af4aadb002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\n",
      " 1. What are the key components of a large language model (LLM) used in autonomous decision-making?\n",
      "\n",
      "Answer\n",
      " The key components of a large language model (LLM) used in autonomous decision-making are planning and reflection/refinement. Planning involves breaking down complex tasks into smaller subgoals and task decomposition using techniques like Chain of Thought (CoT). Reflection and refinement enable the agent to learn from mistakes and improve its performance over time.\n",
      "\n",
      "Question\n",
      " 2. How do LLMs contribute to the development of autonomous agents with natural language understanding and processing capabilities?\n",
      "\n",
      "Answer\n",
      " LLMs contribute to the development of autonomous agents by serving as the agent's brain, complemented by planning, reflection, and refinement components. They enable efficient handling of complex tasks through subgoal decomposition and self-criticism, improving the quality of final results. LLMs also provide a powerful general problem solver with capabilities beyond generating text.\n",
      "\n",
      "Question\n",
      " 3. What role do reinforcement learning algorithms play in integrating LLMs with autonomous agent systems for optimal performance?\n",
      "\n",
      "Answer\n",
      " Reinforcement learning algorithms play a crucial role in integrating LLMs with autonomous agent systems by providing a framework for optimizing performance through trial and error, allowing the agent to learn from its environment and adapt to changing situations. By leveraging reinforcement learning, LLM-powered autonomous agents can refine their planning and decision-making processes, leading to improved overall performance. This integration enables the development of more sophisticated and effective autonomous systems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question, answer in zip(questions, answers):\n",
    "    print(f\"Question\\n {question}\\n\")\n",
    "    print(f\"Answer\\n {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fea07-e031-4435-895b-1bef5770b3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
