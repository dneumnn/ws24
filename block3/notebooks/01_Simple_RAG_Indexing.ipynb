{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e9118f",
   "metadata": {},
   "source": [
    "# WS24 - Intelligente Informationssysteme\n",
    "\n",
    "## Block 3: Retrieval Augmented Generation\n",
    "\n",
    "Build your first simple RAG with LangChain. We follow the LangChain Tutorial \"Build a Retrieval Augmented Generation (RAG) App\" found at <https://python.langchain.com/docs/tutorials/rag/>.\n",
    "\n",
    "**Part 1: Prepare, Split and Indext Knowledge for Storing in Vector Databases**\n",
    "\n",
    "1. Start with data: download and prepare the data you want to add as knowledge. We will extract data from some blog posts found at Lil's Blog (<https://lilianweng.github.io>) into LangChain Documents.\n",
    "2. Split the Documents into Chanks.\n",
    "3. Compute Embedding Vectors and store them in Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0acfc-1a70-4c54-b745-02f26e9c92e1",
   "metadata": {},
   "source": [
    "## 1. Download and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee421b-b80c-4608-97bb-0bcaa999cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Beautiful Soup for Web-Crawling: https://www.crummy.com/software/BeautifulSoup/\n",
    "# Load blog posts from \"https://lilianweng.github.io/posts/\"\n",
    "import bs4\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "url = \"https://lilianweng.github.io/posts/\"\n",
    "\n",
    "# opening connection, grabbing the HTML from the page\n",
    "client = urlopen(url)\n",
    "page_html = client.read()\n",
    "client.close()\n",
    "\n",
    "page_soup = soup(page_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae4b00-959a-48f5-b773-ea6fa736c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_soup.findChildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548fc4f-7a24-4c2f-8494-fd237912239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<a aria-label=\"..\" class=\"entry-link\" href=\"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"></a>\n",
    "blog_posts = []\n",
    "cells = page_soup.find_all(\"a\", attrs={\"class\": \"entry-link\"})\n",
    "for cell in cells:\n",
    "    if type(cell) == bs4.element.Tag:\n",
    "        blog_posts.append( {'label': cell.get('aria-label'), 'link': cell.get('href')} )\n",
    "print(f\"{len(blog_posts)} posts found.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71352a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Beautiful Soup for Web-Crawling: https://www.crummy.com/software/BeautifulSoup/\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# USER_AGENT environment variable\n",
    "# Iterate throug all found blog posts\n",
    "# Use SoupStrainer to keep post title, headers, and content from the full HTML. SoupStrainer is explained at \n",
    "# https://medium.com/codex/using-beautiful-soups-soupstrainer-to-save-time-and-memory-when-web-scraping-ea1dbd2e886f\n",
    "# Use WebBaseLoader to get the requested documents https://python.langchain.com/docs/integrations/document_loaders/web_base/\n",
    "docs = []\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "for blog_post in blog_posts:\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(blog_post['link'],),\n",
    "        bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    )\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f5847-2823-4ea1-8ec4-0609f49b4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a list of LangChain Documents. A Document is an object with some page_content (str) and metadata (dict).\n",
    "print(docs[0].metadata)\n",
    "print(\"Page content:\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b778593-5675-44a3-8c93-6ef3b8ded322",
   "metadata": {},
   "source": [
    "## 2. Split the Documents\n",
    "\n",
    "Now we split each Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n",
    "\n",
    "We split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. \n",
    "\n",
    "The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8364657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                               chunk_overlap=200,\n",
    "                                               length_function=len,\n",
    "                                               #is_separator_regex=False, #not working\n",
    "                                               add_start_index=True,\n",
    "                                               separators=[\"\\n\\n\\n\", \"\\n\"]\n",
    "                                              )\n",
    "\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(all_splits))\n",
    "#print(docs[0].page_content)\n",
    "print(\"===============\")\n",
    "print(all_splits[0].page_content)\n",
    "print(\"---------------\")\n",
    "print(all_splits[1].page_content)\n",
    "print(\"---------------\")\n",
    "print(all_splits[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf03ea-0488-4755-b927-671e610b63b8",
   "metadata": {},
   "source": [
    "## 3. Compute Embedding Vectors and store them in Vector Database\n",
    "\n",
    "Now we need to index our text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\n",
    "\n",
    "We can embed and store all of our document splits in a single command using the Chroma vector store and OpenAIEmbeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329f913-28ac-42f7-b736-3b0614913702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We use nomic embedding porovoded by Ollama\n",
    "# https://ollama.com/library/nomic-embed-text\n",
    "# ollama pull nomic-embed-text\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "## try the embeddings\n",
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "## compute cosine similarity\n",
    "import numpy as np\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / ( np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "similarity = cosine_similarity(np.array(vector_1), np.array(vector_2))\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Chroma DB as vectore database to store all embeddings \n",
    "from langchain_chroma import Chroma\n",
    "vectorstore = Chroma(persist_directory=\"vector_store\", collection_name=\"lils_blogs\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6db3bc-91c1-4c3c-8b26-bdd64235c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f7e86-156f-4e54-b590-1a19246f4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(vectorstore.add_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d025d-d2c9-459c-83bd-36a5116b94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in all_splits:\n",
    "    id = vectorstore.add_documents(documents=[chunk])\n",
    "    #print(f\"chunk added with id {id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e3d7f-2bbe-41c5-8fdf-37c898ea7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Test the vectorstore\n",
    "help(vectorstore.similarity_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea8757-39c1-4abe-8174-add15d232d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_docs = vectorstore.similarity_search(\"What kind of hallucination do LLMs have?\", k=4)\n",
    "for doc in returned_docs:\n",
    "    print(doc.metadata)    \n",
    "    print(doc.page_content)\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f771d5c-f0fe-4e52-b4db-da54555113e4",
   "metadata": {},
   "source": [
    "## LlamaIndex - an alternative Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec25514-e11e-47e6-8925-cd572263635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca866da-8fb1-4af3-908e-e63051a9422a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87730f8-8aeb-4cd1-b048-d5e2d84183f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "documents = [] # list of llama_index documents\n",
    "for doc in docs:\n",
    "    documents.append(Document(text=doc.page_content, metadata=doc.metadata))\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65904c-5c35-43e9-a9e0-ed2d3396bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse text with a preference for complete sentences.\n",
    "#\n",
    "# In general, this class tries to keep sentences and paragraphs together. \n",
    "# Therefore compared to the original TokenTextSplitter, there are less likely \n",
    "# to be hanging sentences or parts of sentences at the end of the node chunk.\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=200,     #words not characters\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c6291-e004-422e-9cb7-662ad35fac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].text)\n",
    "print(\"---------------\")\n",
    "print(nodes[1].text)\n",
    "print(\"---------------\")\n",
    "# Implementation of splitting text that looks at word tokens.print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d067b-3539-40a1-aa46-fa55986e85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=200,     #words not characters\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b5468-15ae-42ff-bbda-a9bd9fe5e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].text)\n",
    "print(\"---------------\")\n",
    "print(nodes[1].text)\n",
    "print(\"---------------\")\n",
    "# Implementation of splitting text that looks at word tokens.print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa1f97-3429-4ddd-8f1f-8db8ea4d9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@bavalpreetsinghh/llama-index-a-comprehensive-guide-for-building-and-querying-document-indexes-27a13bb482a5\n",
    "# https://medium.com/@bavalpreetsinghh/llamaindex-chunking-strategies-for-large-language-models-part-1-ded1218cfd30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba57a17-2d22-4bca-b5a3-82d6f481102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceWindowNodeParser\n",
    "# This component is responsible for parsing documents into individual sentences. \n",
    "# It creates nodes for each sentence, and each node includes a “window” containing the sentences surrounding it. \n",
    "# This means that instead of just having one isolated sentence, you have a context window of sentences around it.\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "help(SentenceWindowNodeParser)\n",
    "\n",
    "#splitter = SentenceWindowNodeParser(\n",
    "#    chunk_size=200,     #words not characters\n",
    "#    chunk_overlap=20,\n",
    "#)\n",
    "#nodes = splitter.get_nodes_from_documents(documents)\n",
    "#print(len(nodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76857d33-2f20-45f5-befb-4954b264cbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
